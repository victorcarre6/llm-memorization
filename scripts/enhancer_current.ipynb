{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e67d9660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorcarre/Code/Projects/llm-memorization/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import subprocess\n",
    "import webbrowser\n",
    "from collections import Counter\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, ttk, Canvas\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import torch\n",
    "import heapq\n",
    "import logging\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import logging as transformers_logging\n",
    "from keybert import KeyBERT\n",
    "import pyperclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886247f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BartLearnedPositionalEmbedding:\n\tsize mismatch for weight: copying a param with shape torch.Size([1026, 768]) from checkpoint, the shape in current model is torch.Size([1026, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m kw_model = KeyBERT()\n\u001b[32m     62\u001b[39m summarizing_model = config.get(\u001b[33m\"\u001b[39m\u001b[33msummarizing_model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel/barthez-orangesum-abstract\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m summarizing_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msummarization\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummarizing_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/pipelines/__init__.py:942\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m model_config = model.config\n\u001b[32m    953\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/pipelines/base.py:292\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     logger.warning(\n\u001b[32m    287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    288\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to load the model with Tensorflow.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m     )\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    294\u001b[39m         model = model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/modeling_utils.py:4574\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4564\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4565\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4567\u001b[39m     (\n\u001b[32m   4568\u001b[39m         model,\n\u001b[32m   4569\u001b[39m         missing_keys,\n\u001b[32m   4570\u001b[39m         unexpected_keys,\n\u001b[32m   4571\u001b[39m         mismatched_keys,\n\u001b[32m   4572\u001b[39m         offload_index,\n\u001b[32m   4573\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4574\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4576\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4580\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4583\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4590\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4592\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4593\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/modeling_utils.py:5031\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5029\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m   5030\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m-> \u001b[39m\u001b[32m5031\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5045\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5047\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[38;5;66;03m# force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\u001b[39;00m\n\u001b[32m   5050\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled():\n\u001b[32m    841\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[43m_load_parameter_into_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    846\u001b[39m     hf_quantizer.create_quantized_param(\n\u001b[32m    847\u001b[39m         model, param, param_name, param_device, state_dict, unexpected_keys\n\u001b[32m    848\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/transformers/modeling_utils.py:731\u001b[39m, in \u001b[36m_load_parameter_into_model\u001b[39m\u001b[34m(model, param_name, tensor)\u001b[39m\n\u001b[32m    729\u001b[39m module, param_type = get_module_from_name(model, param_name)\n\u001b[32m    730\u001b[39m \u001b[38;5;66;03m# This will check potential shape mismatch if skipped before\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mparam_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/Jupyter/lib/python3.13/site-packages/torch/nn/modules/module.py:2593\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2585\u001b[39m         error_msgs.insert(\n\u001b[32m   2586\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2587\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2588\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2589\u001b[39m             ),\n\u001b[32m   2590\u001b[39m         )\n\u001b[32m   2592\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2594\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2595\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2596\u001b[39m         )\n\u001b[32m   2597\u001b[39m     )\n\u001b[32m   2598\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for BartLearnedPositionalEmbedding:\n\tsize mismatch for weight: copying a param with shape torch.Size([1026, 768]) from checkpoint, the shape in current model is torch.Size([1026, 1024])."
     ]
    }
   ],
   "source": [
    "\n",
    "# === INITIALISATION ===\n",
    "\n",
    "# Chargement des variables de configuration\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "config_path = os.path.join(PROJECT_ROOT, \"config.json\")\n",
    "\n",
    "def expand_path(value):\n",
    "    expanded = os.path.expanduser(value)\n",
    "    if not os.path.isabs(expanded):\n",
    "        expanded = os.path.normpath(os.path.join(PROJECT_ROOT, expanded))\n",
    "    return expanded\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_config = json.load(f)\n",
    "\n",
    "    path_keys = {\n",
    "        \"venv_activate_path\",\n",
    "        \"lmstudio_folder_path\",\n",
    "        \"sync_script_path\",\n",
    "        \"project_script_path\",\n",
    "        \"db_path\",\n",
    "        \"stopwords_file_path\"\n",
    "    }\n",
    "\n",
    "    config = {}\n",
    "    for key, value in raw_config.items():\n",
    "        if isinstance(value, str):\n",
    "            if key == \"summarizing_model\" and value == \"model/barthez-orangesum-abstract\":\n",
    "                config[key] = expand_path(value)\n",
    "            elif key in path_keys:\n",
    "                config[key] = expand_path(value)\n",
    "            else:\n",
    "                config[key] = value\n",
    "        else:\n",
    "            config[key] = value\n",
    "    return config\n",
    "\n",
    "config = load_config(config_path)\n",
    "\n",
    "stopwords_path = config.get(\"stopwords_file_path\", \"stopwords_fr.json\")\n",
    "with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    french_stop_words = set(json.load(f))\n",
    "\n",
    "combined_stopwords = ENGLISH_STOP_WORDS.union(french_stop_words)\n",
    "\n",
    "# Masquage des avertissements\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "torch._C._log_api_usage_once = lambda *args, **kwargs: None\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unfeasible length constraints\", category=UserWarning, module=\"transformers.generation.utils\")\n",
    "\n",
    "# Connexion à la base SQLite\n",
    "conn = sqlite3.connect(config[\"db_path\"])\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Initialisation des modèles\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "kw_model = KeyBERT()\n",
    "summarizing_model = config.get(\"summarizing_model\", \"model/barthez-orangesum-abstract\")\n",
    "summarizing_pipeline = pipeline(task=\"summarization\", model=summarizing_model, framework=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7be31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FONCTIONS PRINCIPALES ===\n",
    "\n",
    "# Synchronisation des conversations\n",
    "def sync_conversations(config, label_status):\n",
    "    sync_path = config.get(\"sync_script_path\")\n",
    "    if not sync_path:\n",
    "        label_status.config(text=\"sync_script_path introuvable.\")\n",
    "        return False\n",
    "    try:\n",
    "        subprocess.run([\"python3\", sync_path], check=True)\n",
    "        label_status.config(text=\"Synchronisation terminée.\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        label_status.config(text=\"Erreur lors de la synchronisation.\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "        label_status.config(text=\"Script de synchronisation introuvable.\")\n",
    "        return False\n",
    "\n",
    "# Pércuteur\n",
    "def on_ask():\n",
    "    question = entry_question.get(\"1.0\", \"end-1c\")\n",
    "    if not question.strip():\n",
    "        update_status(\"⚠️ Merci de saisir une question.\", error=True)\n",
    "        return\n",
    "    update_status(\"⚙️ Traitement en cours...\")\n",
    "    root.update()\n",
    "    try:\n",
    "        context = get_relevant_context(question, limit=context_count_var.get()) #\", limit=context_count_var.get()\" ajoutée slider contexte\n",
    "        prompt = generate_prompt_paragraph(context, question)\n",
    "        pyperclip.copy(prompt)\n",
    "        text_output.delete('1.0', tk.END)\n",
    "        text_output.insert(tk.END, prompt)\n",
    "        \n",
    "        # Calcul des métriques\n",
    "        context_count = len(context)\n",
    "        token_count = len(prompt.split())\n",
    "        \n",
    "        update_status(\n",
    "            f\"Prompt généré ({token_count} tokens) | Contexte utilisé : {context_count} éléments\",\n",
    "            success=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        update_status(f\"❌ Erreur : {str(e)}\", error=True)\n",
    "\n",
    "# === CONTEXTE ===\n",
    "\n",
    "# Récupération des mots-clés de la question initiale\n",
    "root = tk.Tk()\n",
    "keyword_count_var = tk.IntVar(value=5)\n",
    "context_count_var = tk.IntVar(value=3)\n",
    "multiplier = config.get(\"keyword_multiplier\", 2)\n",
    "\n",
    "def extract_keywords(text, top_n=None):\n",
    "    if top_n is None:\n",
    "        top_n = keyword_count_var.get()\n",
    "\n",
    "    # Extraction brute avec KeyBERT\n",
    "    raw_keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 1),\n",
    "        stop_words=list(combined_stopwords),\n",
    "        top_n=top_n * multiplier)\n",
    "\n",
    "    stopwords_set = set(combined_stopwords)\n",
    "\n",
    "    tokens = re.findall(r'\\b[a-zA-Z\\-]{3,}\\b', text.lower())\n",
    "    token_freq = Counter([tok for tok in tokens if tok not in stopwords_set])\n",
    "\n",
    "    # Fonction de validation rapide des mots clés\n",
    "    def is_valid_kw(kw):\n",
    "        return (\n",
    "            kw not in stopwords_set and\n",
    "            len(kw) > 2 and\n",
    "            kw.isalpha() or '-' in kw\n",
    "        )\n",
    "\n",
    "    # Tri par fréquence dans le texte #  /!\\ ==peu entrainer des doublons== /!\\\n",
    "    filtered_raw = []\n",
    "    for kw, weight in raw_keywords:\n",
    "        kw_clean = kw.lower().strip()\n",
    "        if is_valid_kw(kw_clean):\n",
    "            freq = token_freq.get(kw_clean, 0)\n",
    "            filtered_raw.append((freq, kw_clean, weight))\n",
    "\n",
    "    top_filtered = heapq.nlargest(top_n, filtered_raw, key=lambda x: x[0])\n",
    "\n",
    "    seen = set()\n",
    "    filtered_keywords = []\n",
    "    for freq, kw_clean, weight in top_filtered:\n",
    "        if kw_clean not in seen:\n",
    "            seen.add(kw_clean)\n",
    "            filtered_keywords.append((kw_clean, weight, freq))\n",
    "\n",
    "    return filtered_keywords\n",
    "\n",
    "# Requête SQL en fonction des mots-clés extraits\n",
    "def get_relevant_context(user_question, limit=None):\n",
    "    if limit is None:\n",
    "        limit = context_count_var.get()  # Récupération dynamique si pas de limite donnée\n",
    "    keywords = extract_keywords(user_question)\n",
    "    if not keywords:\n",
    "        return []\n",
    "    placeholders = ', '.join(['?'] * len(keywords))\n",
    "    query = f'''\n",
    "        SELECT c.id, c.user_input, c.llm_output, c.timestamp, k.keyword\n",
    "        FROM conversations c\n",
    "        JOIN keywords k ON c.id = k.conversation_id\n",
    "        WHERE k.keyword IN ({placeholders})\n",
    "    '''\n",
    "    keyword_strings = [kw[0] for kw in keywords]\n",
    "    cur.execute(query, keyword_strings)\n",
    "    rows = cur.fetchall()\n",
    "    match_counts = {}\n",
    "    context_data = {}\n",
    "    for convo_id, user_input, llm_output, timestamp, keyword in rows:\n",
    "        if convo_id not in match_counts:\n",
    "            match_counts[convo_id] = set()\n",
    "            context_data[convo_id] = (user_input, llm_output, timestamp)\n",
    "        match_counts[convo_id].add(keyword)\n",
    "    scored_contexts = [(convo_id, len(matched_keywords)) for convo_id, matched_keywords in match_counts.items()] # Calcul du score (nombre de mots-clés distincts en commun)\n",
    "    sorted_contexts = sorted(scored_contexts, key=lambda x: x[1], reverse=True) # Tri décroissant par score\n",
    "    filtered_context = [context_data[convo_id] for convo_id, score in sorted_contexts[:limit]]  # Sélection des meilleurs résultats selon la limite\n",
    "    return filtered_context\n",
    "\n",
    "# Nettoyage du texte\n",
    "def nlp_clean_text(text, max_chunk_size=500):\n",
    "\n",
    "    text = re.sub(r'```(?:python)?\\s*.*?```', '', text, flags=re.DOTALL)\n",
    "    chunks, current_chunk, current_length = [], [], 0\n",
    "\n",
    "    for sent in nlp(text).sents:\n",
    "        s = sent.text.strip()\n",
    "        if len(s) < 20:\n",
    "            continue\n",
    "        if current_length + len(s) < max_chunk_size:\n",
    "            current_chunk.append(s)\n",
    "            current_length += len(s)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [s]\n",
    "            current_length = len(s)\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return \" \".join(chunks[:3])  # limite à 3 blocs maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONSTRUCTION DU PROMPT ===\n",
    "\n",
    "# Compression du contexte extrait\n",
    "def summarize(text, focus_terms=None, max_length=1024):\n",
    "    transformers_logging.set_verbosity_error()\n",
    "    try:\n",
    "        # Filtrage des phrases importantes si focus_terms donné\n",
    "        if focus_terms:\n",
    "            sentences = [s for s in text.split('.') \n",
    "                        if any(term.lower() in s.lower() for term in focus_terms)]\n",
    "            text = '. '.join(sentences)[:2000] or text[:2000]\n",
    "\n",
    "        # Résumé avec le texte filtré\n",
    "        result = summarizing_pipeline(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            min_length=max_length // 2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        return nlp_clean_text(result[0]['summary_text'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur summarization : {e}\")\n",
    "        return text[:max_length] + \"... [résumé tronqué]\"\n",
    "    \n",
    "# Construction du prompt\n",
    "def generate_prompt_paragraph(context, question, target_tokens=1000):\n",
    "    if not context:\n",
    "        return f\"{question}\"\n",
    "\n",
    "    # 1. Prétraitement\n",
    "    processed_items = []\n",
    "    for item in context[:3]:  # Nombre max d'éléments dans le contexte\n",
    "        try:\n",
    "            # Extraction sécurisée\n",
    "            user_input = str(item[0])[:300]  # Troncature des questions longues\n",
    "            llm_output = str(item[1])\n",
    "            keyword = str(item[5]) if len(item) > 5 and str(item[3]).strip() not in {\"\", \"none\", \"null\", \"1\", \"2\", \"3\"} else None\n",
    "\n",
    "            # Summarization, netooyage, segmentation\n",
    "            summary = nlp_clean_text(summarize(llm_output))\n",
    "            processed_items.append({\n",
    "                'question': user_input,\n",
    "                'summary': summary,\n",
    "                'keyword': keyword if keyword else None\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur traitement item : {e}\")\n",
    "            continue\n",
    "\n",
    "    if not processed_items:\n",
    "        return question\n",
    "\n",
    "    # 2. Construction du prompt\n",
    "    parts = []\n",
    "\n",
    "    # Partie questions\n",
    "    if processed_items:\n",
    "        questions = [f\"'{item['question']}'\" for item in processed_items]\n",
    "        if len(questions) == 1:\n",
    "            parts.append(f\"Tes discussions avec l'utilisateur t'ont amené à répondre à cette question : {questions[0]}\")\n",
    "        else:\n",
    "            *init, last = questions\n",
    "            parts.append(f\"Tes discussions avec l'utilisateur t'ont amené à répondre à ces questions :  {', '.join(init)}, et enfin {last}\")\n",
    "\n",
    "    # Partie mots-clés\n",
    "    keywords = {item['keyword'] for item in processed_items if item['keyword']}\n",
    "    if keywords:\n",
    "        parts.append(f\"Mots-clés pertinents : {', '.join(sorted(keywords))}\")\n",
    "\n",
    "    # Partie résumés\n",
    "    if processed_items:\n",
    "        summaries = [f\"- {item['summary']}\" for item in processed_items]\n",
    "        parts.append(\"Ces intéractions vous ont amené à discuter de ces sujets :\\n\" + \"\\n\".join(summaries))\n",
    "\n",
    "    # Question actuelle\n",
    "    parts.append(f\"Réponds maintenant à cette question, dans le contexte de vos discussions précédentes : {question}\")\n",
    "\n",
    "    return \"\\n\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INTERFACE TKINTER ===\n",
    "\n",
    "def update_status(message, error=False, success=False):\n",
    "    \"\"\"Met à jour le label de statut avec style approprié\"\"\"\n",
    "    label_status.config(text=message)\n",
    "    if error:\n",
    "        label_status.config(foreground='#ff6b6b')\n",
    "    elif success:\n",
    "        label_status.config(foreground='#599258')\n",
    "    else:\n",
    "        label_status.config(foreground='white')\n",
    "\n",
    "def open_github(event):\n",
    "    webbrowser.open_new(\"https://github.com/victorcarre6\")\n",
    "\n",
    "def show_help():\n",
    "    help_text = (\n",
    "        \"LLM Memorization and Prompt Enhancer — Aide\\n\\n\"\n",
    "        \"• Synchroniser les conversations : ajoute les nouveaux échanges depuis LM Studio à la base de données.\\n\\n\"\n",
    "        \"• Générer prompt : extrait les mots-clés de votre question, cherche des conversations similaires dans votre base SQL, puis compresse les informations avec un LLM local.\\n\\n\"\n",
    "        \"Le prompt final est affiché puis automatiquement copié dans votre presse-papier !\\n\\n\"\n",
    "        \"Pour en savoir plus, obtenir plus d'informations à propos d'un éventuel bloquage des scripts, ou me contacter, visitez : github.com/victorcarre6/llm-memorization\"\n",
    "    )\n",
    "    help_window = tk.Toplevel(root)\n",
    "    help_window.title(\"Aide\")\n",
    "    help_window.geometry(\"500x300\")\n",
    "    help_window.configure(bg=\"#323232\")\n",
    "\n",
    "    text_widget = tk.Text(help_window, wrap=tk.WORD, font=(\"Segoe UI\", 8))\n",
    "    text_widget.insert(tk.END, help_text)\n",
    "    text_widget.config(state=tk.DISABLED)\n",
    "    text_widget.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    ok_button = tk.Button(help_window, text=\"Fermer\", command=help_window.destroy)\n",
    "    ok_button.pack(pady=10)\n",
    "\n",
    "# === CONFIGURATION DE L'INTERFACE ===\n",
    "root.title(\"LLM Memorization and Prompt Enhancer\")\n",
    "root.geometry(\"1000x325\")\n",
    "root.configure(bg=\"#323232\")\n",
    "\n",
    "# Style global unique\n",
    "style = ttk.Style(root)\n",
    "style.theme_use('clam')\n",
    "\n",
    "# Configuration du style\n",
    "style_config = {\n",
    "    'Green.TButton': {\n",
    "        'background': '#599258',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 11),\n",
    "        'padding': 4\n",
    "    },\n",
    "    'Bottom.TButton': {\n",
    "        'background': '#599258',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 9),\n",
    "        'padding': 2\n",
    "    },\n",
    "    'Blue.TLabel': {\n",
    "        'background': '#323232',\n",
    "        'foreground': '#599258',\n",
    "        'font': ('Segoe UI', 8, 'italic underline'),\n",
    "        'padding': 0\n",
    "    },\n",
    "    'TLabel': {\n",
    "        'background': '#323232',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TEntry': {\n",
    "        'fieldbackground': '#FDF6EE',\n",
    "        'foreground': 'black',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TFrame': {\n",
    "        'background': '#323232'\n",
    "    },\n",
    "    'Status.TLabel': {\n",
    "        'background': '#323232',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TNotebook': {\n",
    "        'background': '#323232',\n",
    "        'borderwidth': 0\n",
    "    },\n",
    "    'TNotebook.Tab': {\n",
    "        'background': '#2a2a2a',\n",
    "        'foreground': 'white',\n",
    "        'padding': (10, 4)\n",
    "    },\n",
    "    'Custom.Treeview': {\n",
    "        'background': '#2a2a2a',\n",
    "        'foreground': 'white',\n",
    "        'fieldbackground': '#2a2a2a',\n",
    "        'font': ('Segoe UI', 10),\n",
    "        'bordercolor': '#323232',\n",
    "        'borderwidth': 0,\n",
    "    },\n",
    "    'Custom.Treeview.Heading': {\n",
    "        'background': '#323232',\n",
    "        'foreground': '#599258',\n",
    "        'font': ('Segoe UI', 11, 'bold'),\n",
    "        'relief': 'flat'\n",
    "    }\n",
    "}\n",
    "\n",
    "for style_name, app_config in style_config.items():\n",
    "    style.configure(style_name, **app_config)\n",
    "\n",
    "style.map('Green.TButton',\n",
    "          background=[('active', '#457a3a'), ('pressed', '#2e4a20')],\n",
    "          foreground=[('disabled', '#d9d9d9')])\n",
    "\n",
    "style.map(\"TNotebook.Tab\",\n",
    "          background=[(\"selected\", \"#323232\"), (\"active\", \"#2a2a2a\")],\n",
    "          foreground=[(\"selected\", \"white\"), (\"active\", \"white\")])\n",
    "\n",
    "\n",
    "style.map('Bottom.TButton',\n",
    "          background=[('active', '#457a3a'), ('pressed', '#2e4a20')],\n",
    "          foreground=[('disabled', '#d9d9d9')])\n",
    "\n",
    "# Widgets principaux\n",
    "main_frame = ttk.Frame(root, style='TFrame')\n",
    "main_frame.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Section question - Centrée\n",
    "question_header = ttk.Frame(main_frame, style='TFrame')\n",
    "question_header.pack(fill='x', pady=(0, 1))\n",
    "ttk.Label(question_header, text=\"Poser la question :\").pack(expand=True)\n",
    "\n",
    "question_frame = tk.Frame(main_frame, bg=\"#323232\")\n",
    "question_frame.pack(pady=(0, 5), fill='x', expand=True)\n",
    "\n",
    "entry_question = tk.Text(question_frame, height=4, width=80, wrap=\"word\", font=('Segoe UI', 11))\n",
    "entry_question.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# Configuration du style pour la scrollbar\n",
    "style = ttk.Style()\n",
    "style.configure(\"Vertical.TScrollbar\",\n",
    "    troughcolor='#FDF6EE',\n",
    "    background='#C0C0C0',\n",
    "    darkcolor='#C0C0C0',\n",
    "    lightcolor='#C0C0C0',\n",
    "    bordercolor='#FDF6EE',\n",
    "    arrowcolor='black',\n",
    "    relief='flat')\n",
    "\n",
    "scrollbar = ttk.Scrollbar(\n",
    "    question_frame,\n",
    "    orient=\"vertical\",\n",
    "    command=entry_question.yview,\n",
    "    style=\"Vertical.TScrollbar\"  # Application du style\n",
    ")\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "\n",
    "entry_question.config(yscrollcommand=scrollbar.set)\n",
    "entry_question.bind(\"<Return>\", lambda event: on_ask())\n",
    "\n",
    "# Frame horizontale principale\n",
    "control_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "control_frame.pack(fill='x', pady=(0, 10), padx=5)\n",
    "\n",
    "# Sliders\n",
    "slider_keywords_frame = ttk.Frame(control_frame, style='TFrame')\n",
    "slider_keywords_frame.grid(row=0, column=0, sticky='w')\n",
    "\n",
    "label_keyword_count = ttk.Label(slider_keywords_frame, text=f\"Nombre de mots-clés : {keyword_count_var.get()}\", style='TLabel')\n",
    "label_keyword_count.pack(anchor='w')\n",
    "\n",
    "slider_keywords = ttk.Scale(\n",
    "    slider_keywords_frame,\n",
    "    from_=1,\n",
    "    to=15,\n",
    "    orient=\"horizontal\",\n",
    "    variable=keyword_count_var,\n",
    "    length=180,\n",
    "    command=lambda val: label_keyword_count.config(text=f\"Nombre de mots-clés : {int(float(val))}\")\n",
    ")\n",
    "slider_keywords.pack(anchor='w')\n",
    "\n",
    "slider_context_frame = ttk.Frame(control_frame, style='TFrame')\n",
    "slider_context_frame.grid(row=0, column=1, padx=20, sticky='w')\n",
    "\n",
    "label_contexts_count = ttk.Label(slider_context_frame, text=f\"Nombre de contextes : {context_count_var.get()}\", style='TLabel')\n",
    "label_contexts_count.pack(anchor='w')\n",
    "\n",
    "slider_contexts = ttk.Scale(\n",
    "    slider_context_frame,\n",
    "    from_=1,\n",
    "    to=5,\n",
    "    orient=tk.HORIZONTAL,\n",
    "    variable=context_count_var,\n",
    "    length=170,\n",
    "    command=lambda val: label_contexts_count.config(text=f\"Nombre de contextes : {int(float(val))}\")\n",
    ")\n",
    "slider_contexts.pack(anchor='w')\n",
    "\n",
    "# Boutons synchronisation et percuteur\n",
    "button_frame = ttk.Frame(control_frame, style='TFrame')\n",
    "button_frame.grid(row=0, column=2, sticky='e')\n",
    "\n",
    "sync_button = ttk.Button(button_frame, text=\"Synchroniser les conversations\", \n",
    "                         command=sync_conversations, style='Green.TButton')\n",
    "sync_button.pack(side='left', padx=5)\n",
    "\n",
    "btn_ask = ttk.Button(button_frame, text=\"Générer prompt\", command=on_ask, style='Green.TButton')\n",
    "btn_ask.pack(side='left', padx=5)\n",
    "control_frame.grid_columnconfigure(2, weight=1)\n",
    "\n",
    "# Zone de sortie étendable\n",
    "output_expanded = tk.BooleanVar(value=False)\n",
    "\n",
    "def toggle_output():\n",
    "    \"\"\"Basculer l'affichage de la zone de sortie et ajuster la taille de la fenêtre\"\"\"\n",
    "    if output_expanded.get():\n",
    "        text_output.pack_forget()\n",
    "        toggle_btn.config(text=\"▼ Afficher le résultat\")\n",
    "        output_expanded.set(False)\n",
    "        root.geometry(\"1000x325\")\n",
    "    else:\n",
    "        text_output.pack(fill=tk.BOTH, expand=True, pady=(5, 0))\n",
    "        toggle_btn.config(text=\"▲ Masquer le résultat\")\n",
    "        output_expanded.set(True)\n",
    "        root.geometry(\"1000x750\")\n",
    "\n",
    "output_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "output_frame.pack(fill=tk.BOTH, expand=True, pady=(0, 10))\n",
    "\n",
    "# Bouton pour étendre/cacher\n",
    "toggle_btn = ttk.Button(\n",
    "    output_frame,\n",
    "    text=\"▼ Afficher les résultats\",\n",
    "    command=toggle_output,\n",
    "    style='Green.TButton'  # Utilise le même style que tes autres boutons\n",
    ")\n",
    "toggle_btn.pack(fill=tk.X, pady=(0, 5))\n",
    "\n",
    "text_output = scrolledtext.ScrolledText(\n",
    "    output_frame, \n",
    "    width=100, \n",
    "    height=20,\n",
    "    font=('Segoe UI', 11), \n",
    "    wrap=tk.WORD, \n",
    "    bg=\"#FDF6EE\", \n",
    "    fg=\"black\", \n",
    "    insertbackground=\"black\"\n",
    ")\n",
    "\n",
    "def show_infos():\n",
    "    info_window = tk.Toplevel(root)\n",
    "    info_window.title(\"Détails sur le prompt généré\")\n",
    "    info_window.geometry(\"750x725\")\n",
    "    container = tk.Frame(info_window, bg=\"#323232\")\n",
    "    container.pack(fill=\"both\", expand=True)\n",
    "\n",
    "    info_window.transient(root)\n",
    "    info_window.grab_set()\n",
    "\n",
    "    question = entry_question.get(\"1.0\", tk.END).strip()\n",
    "    if not question:\n",
    "        update_status(\"⚠️ Posez une question d'abord pour accéder aux infos.\", error=True)\n",
    "        info_window.destroy()\n",
    "        return\n",
    "\n",
    "    keywords = extract_keywords(question)  # [(kw, weight, freq), ...]\n",
    "    context = get_relevant_context(question)  # Liste de tuples (user_input, llm_output, timestamp)\n",
    "\n",
    "    notebook = ttk.Notebook(container, style=\"TNotebook\")\n",
    "    notebook.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
    "\n",
    "    single_tab = ttk.Frame(notebook, style=\"TFrame\")\n",
    "    notebook.add(single_tab, text=\"Mots-clés & Stats\")\n",
    "\n",
    "    # Calcul des fréquences dans les contextes\n",
    "    full_text_context = \" \".join(user_input + \" \" + llm_output for user_input, llm_output, _ in context).lower()\n",
    "    token_list = re.findall(r'\\b[a-zA-Z\\-]{3,}\\b', full_text_context)\n",
    "\n",
    "    word_counts = {}\n",
    "    for kw, _, _ in keywords:\n",
    "        count = token_list.count(kw.lower())\n",
    "        word_counts[kw] = count\n",
    "\n",
    "    # --- Histogramme ---\n",
    "    fig, ax = plt.subplots(figsize=(6, 4), dpi=100)\n",
    "    bars = ax.bar(word_counts.keys(), word_counts.values(), color='#599258')\n",
    "\n",
    "    ax.set_facecolor(\"#323232\")\n",
    "    fig.patch.set_facecolor(\"#323232\")\n",
    "    ax.set_title(\"Fréquence des mots-clés dans les contextes\", color=\"white\", fontsize=10)\n",
    "    ax.set_ylabel(\"Occurrences\", color=\"white\", fontsize=10)\n",
    "    ax.tick_params(axis='x', labelrotation=45, colors=\"white\", labelsize=10)\n",
    "    ax.tick_params(axis='y', colors=\"white\", labelsize=10)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('white')\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    canvas = FigureCanvasTkAgg(fig, master=single_tab)\n",
    "    canvas.draw()\n",
    "    canvas.get_tk_widget().pack(fill=tk.BOTH, expand=False, padx=10, pady=(10, 0))\n",
    "\n",
    "    # --- Tableau Treeview ---\n",
    "    cols = ('Mot-clé', 'Poids', 'Occurrences dans contextes')\n",
    "    tree = ttk.Treeview(single_tab, columns=cols, show='headings', height=10, style='Custom.Treeview')\n",
    "    for col in cols:\n",
    "        tree.heading(col, text=col)\n",
    "        tree.column(col, width=150)\n",
    "    tree.pack(expand=False, fill='both', padx=10, pady=(10, 0))\n",
    "\n",
    "    # Alternance des lignes\n",
    "    tree.tag_configure('oddrow', background='#2a2a2a')\n",
    "    tree.tag_configure('evenrow', background='#383838')\n",
    "\n",
    "    for i, (kw, weight, _) in enumerate(keywords):\n",
    "        freq_in_context = word_counts.get(kw, 0)\n",
    "        tag = 'evenrow' if i % 2 == 0 else 'oddrow'\n",
    "        tree.insert('', tk.END, values=(kw, f\"{weight:.3f}\", freq_in_context), tags=(tag,))\n",
    "\n",
    "    # --- Bouton copier ---\n",
    "    btn_copy = ttk.Button(single_tab, text=\"Copier mots-clés\", command=lambda: (\n",
    "        root.clipboard_clear(),\n",
    "        root.clipboard_append(\",\".join([kw for kw, _, _ in keywords])),\n",
    "    ), style='Bottom.TButton')\n",
    "    btn_copy.pack(pady=10)\n",
    "\n",
    "    # === Onglet 2 : Contexte ===\n",
    "    context_frame = ttk.Frame(notebook, style=\"TFrame\")\n",
    "    notebook.add(context_frame, text=\"Contextes\")\n",
    "\n",
    "    # Utiliser Frame classique pour labels avec bg\n",
    "    lbl_container = tk.Frame(context_frame, bg=\"#323232\")\n",
    "    lbl_container.pack(fill=\"both\", expand=True)\n",
    "\n",
    "    tk.Label(lbl_container, text=\"Questions contextuelles :\", fg=\"white\", bg=\"#323232\", font=(\"Segoe UI\", 10, \"bold\")).pack(pady=5)\n",
    "    tk.Label(lbl_container,\n",
    "             text=\"Légende :\\n- 1 mot clef : rouge\\n- 2 ou 3 : orange\\n- >3 : vert\",\n",
    "             fg=\"white\", bg=\"#323232\", justify=\"left\", wraplength=700, font=(\"Segoe UI\", 8, \"italic\")).pack(anchor=\"w\", padx=10, pady=(0, 10))\n",
    "\n",
    "    for item in context:\n",
    "        q_text = item[0]\n",
    "        extracted = extract_keywords(q_text)\n",
    "        shared = len(set(extracted) & set(keywords))\n",
    "        color = \"#ff6b6b\" if shared <= 1 else \"#ffb347\" if shared <= 3 else \"#7CFC00\"\n",
    "        tk.Label(\n",
    "            lbl_container,\n",
    "            text=q_text[:100] + (\"...\" if len(q_text) > 100 else \"\"),\n",
    "            fg=color, bg=\"#323232\", wraplength=700\n",
    "        ).pack(anchor=\"w\", padx=10)\n",
    "\n",
    "\n",
    "    # === Onglet 3 : Carte mentale ===\n",
    "    mindmap_frame = ttk.Frame(notebook, style=\"TFrame\")\n",
    "    notebook.add(mindmap_frame, text=\"Carte mentale\")\n",
    "\n",
    "    canvas = tk.Canvas(mindmap_frame, bg=\"#323232\", highlightthickness=0)\n",
    "    canvas.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "    # Préparer données pour carte mentale\n",
    "    questions_list = [item[0] for item in context]  # liste des questions contextuelles\n",
    "    keywords_per_question = [extract_keywords(q) for q in questions_list]\n",
    "\n",
    "    import math\n",
    "    n = len(questions_list)\n",
    "    center_x, center_y = 350, 300\n",
    "    radius = 250\n",
    "    nodes_positions = []\n",
    "\n",
    "    # Positionner les nœuds en cercle\n",
    "    for i in range(n):\n",
    "        angle = 2 * math.pi * i / n\n",
    "        x = center_x + radius * math.cos(angle)\n",
    "        y = center_y + radius * math.sin(angle)\n",
    "        nodes_positions.append((x, y))\n",
    "\n",
    "    # Dessiner les liens entre nœuds\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if set(keywords_per_question[i]) & set(keywords_per_question[j]):\n",
    "                x1, y1 = nodes_positions[i]\n",
    "                x2, y2 = nodes_positions[j]\n",
    "                canvas.create_line(x1, y1, x2, y2, fill=\"#7CFC00\", width=1)\n",
    "\n",
    "    # Dessiner les nœuds (cercles)\n",
    "    radius_node = 15\n",
    "\n",
    "    _tooltip = None\n",
    "    def show_tooltip(event, text):\n",
    "        nonlocal _tooltip\n",
    "        if _tooltip:\n",
    "            canvas.delete(_tooltip)\n",
    "            _tooltip = None\n",
    "        x, y = event.x + 10, event.y + 10\n",
    "        _tooltip = canvas.create_text(x, y, text=text, anchor=\"nw\", fill=\"white\", font=(\"Segoe UI\", 9), width=300, tags=\"tooltip\")\n",
    "\n",
    "    def hide_tooltip(event):\n",
    "        nonlocal _tooltip\n",
    "        if _tooltip:\n",
    "            canvas.delete(_tooltip)\n",
    "            _tooltip = None\n",
    "\n",
    "    for i, (x, y) in enumerate(nodes_positions):\n",
    "        node = canvas.create_oval(x-radius_node, y-radius_node, x+radius_node, y+radius_node, fill=\"#599258\", outline=\"\")\n",
    "        # Bind survol pour tooltip\n",
    "        canvas.tag_bind(node, \"<Enter>\", lambda e, q=questions_list[i]: show_tooltip(e, q))\n",
    "        canvas.tag_bind(node, \"<Leave>\", hide_tooltip)\n",
    "\n",
    "\n",
    "# Barre de statut ET boutons - Frame horizontal unifié\n",
    "status_buttons_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "status_buttons_frame.pack(fill=tk.X, pady=(5, 2))\n",
    "\n",
    "# Statut à gauche\n",
    "label_status = ttk.Label(\n",
    "    status_buttons_frame,\n",
    "    text=\"Prêt\",\n",
    "    style='Status.TLabel',\n",
    "    foreground='white',\n",
    "    anchor='w'  # Alignement à gauche plus naturel\n",
    ")\n",
    "label_status.pack(side=tk.LEFT, anchor='w')\n",
    "\n",
    "# Boutons à droite - dans le même frame horizontal\n",
    "right_buttons = ttk.Frame(status_buttons_frame, style='TFrame')\n",
    "right_buttons.pack(side=tk.RIGHT, anchor='e')\n",
    "\n",
    "btn_info = ttk.Button(right_buttons, text=\"Détails\", style='Bottom.TButton', command=show_infos, width=8)\n",
    "btn_info.pack(side=tk.TOP, pady=(0, 3))\n",
    "\n",
    "btn_help = ttk.Button(right_buttons, text=\"Aide\", style='Bottom.TButton', command=show_help, width=8)\n",
    "btn_help.pack(side=tk.TOP)\n",
    "\n",
    "# Footer - inchangé\n",
    "footer_frame = ttk.Frame(root, style='TFrame')\n",
    "footer_frame.pack(side=tk.BOTTOM, fill=tk.X, padx=10, pady=(0, 5))\n",
    "\n",
    "dev_label = ttk.Label(footer_frame, text=\"Développé par Victor Carré —\", style='TLabel', font=('Segoe UI', 8))\n",
    "dev_label.pack(side=tk.LEFT)\n",
    "\n",
    "github_link = ttk.Label(footer_frame, text=\"GitHub\", style='Blue.TLabel', cursor=\"hand2\")\n",
    "github_link.pack(side=tk.LEFT)\n",
    "github_link.bind(\"<Button-1>\", open_github)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62681f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 492. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots-clés extraits de la question : ['photocatalyse', 'pharmaceutique', 'drug', 'discovery', 'actuellement', 'secteur']\n",
      "[1] Texte original : Le processus de découverte de médicaments, ou \"drug discovery\", est un processus complexe et coûteux impliquant plusieurs étapes pour identifier de nouveaux médicaments potentiels. Voici un aperçu des ...\n",
      "[4] Appel au summarizer (taille texte : 2782)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/Users/victorcarre/venvs/Jupyter/lib/python3.13/site-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (512) is larger than the maximum possible length (257). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1024, but your input_length is only 533. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=266)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Résultat brut : [{'summary_text': \"Le processus de drug discovery est un des plus importants et coûteux dans le secteur pharmaceutique. Voici les étapes clés du processus : 1. Identification des cibles : La première étape consiste à identifier les protéines ou les mécanismes cellulaires impliqués dans une maladie donnée ; ensuite, les scientifiques développent des molécules qui interagissent avec la cible et modulent son activité (chimiques, biologiques, cytogènes, etc). 3. Synthèse de composés : Les molécules conçues sont then synthétisées et testées en laboratoire pour déterminer leur efficacité et leur sécurité. 7. Approbation réglementaire : Si un médicament réussit ses essais cliniques, l'entreprise qui le développe doit soumettre une demande de ses filiales à une agence réglementaire, telle que la FDA en Europe ou l'Agence européenne de l'assurance maladie (Agence de la santé) en Europe (Agence des médicaments ou des produits de la sécurité du médicament) a une autorisation de la FDA pour l'agence du médicament (ANSA) pour les États-Unis ou les autorités sanitaires (FeMA) pour la FDA (États-Unis) pour le produit pharmaceutique) pour une autorisation d'utiliser l'a approuvé pour la commercialisation ou pour la mise sur l'ensemble de la population ou des patients pour la reproduction des patients. C'est le cas :\"}]\n",
      "Avant segmentation : 193 mots\n",
      "[1] Texte original : Les photocatalyseurs rédox sont largement utilisés dans diverses applications industrielles pour catalyser des réactions chimiques à travers le transfert d'électrons induit par la lumière. Voici quelq ...\n",
      "[4] Appel au summarizer (taille texte : 2447)\n",
      "[5] Résultat brut : [{'summary_text': \"Le dioxyde de titane est l'un des photocatalyseurs les plus répandus et le plus étudié en raison de sa stabilité, de sa toxicité faible et de ses propriétés optiques uniques. Il peut être utilisé pour des applications telles que la dégradation des polluants, la génération d'hydrogène et l'aménagement des sols. 6. g-C3N4 (Graphite de carbone nitrure) : Le graphite d'azote nitrure est un photocatalyse à base de carbure organique, qui présente des propriétés catalytiques intéressantes pour la dégradation de polluants et de dégradation des métaux. Ces exemples de matériaux peuvent être utilisés en fonction des applications spécifiques, mais il existe de nombreux autres matériaux qui ne sont pas les plus utilisés, tels que le titane ou le ZnO, utilisés en majorité pour les métaux, qui présentent des différences de leurs propriétés optiques, comme le nickel ou les nitrate de titane ou les oxydes de fer fer ferro-solide de zinc.L'Oxide de zinc ou le chrome, utilisés dans la composition des métaux, utilisés pour la composition de leur composition ou les métaux communs, utilisés par exemple, pour la photoélectrochimie.Figure de cadmium ou les germaniques, pour l'optimisation des flux d'\"}]\n",
      "Avant segmentation : 146 mots\n",
      "Questions antérieures : 'Comment faire du \"drug discovery\" dans le secteur pharmaceutique ?', et enfin 'Quels sont les photocatalyseurs rédox les plus utilisés dans l'industrie ?'\n",
      "Contexte pertinent :\n",
      "- Le processus de drug discovery est un des plus importants et coûteux dans le secteur pharmaceutique. Voici les étapes clés du processus : 1. Identification des cibles : La première étape consiste à identifier les protéines ou les mécanismes cellulaires impliqués dans une maladie donnée ; ensuite, les scientifiques développent des molécules qui interagissent avec la cible et modulent son activité (chimiques, biologiques, cytogènes, etc). Synthèse de composés : Les molécules conçues sont then synthétisées et testées en laboratoire pour déterminer leur efficacité et leur sécurité. Approbation réglementaire : Si un médicament réussit ses essais cliniques, l'entreprise qui le développe doit soumettre une demande de ses filiales à une agence réglementaire, telle que la FDA en Europe ou l'Agence européenne de l'assurance maladie (Agence de la santé) en Europe (Agence des médicaments ou des produits de la sécurité du médicament) a une autorisation de la FDA pour l'agence du médicament (ANSA) pour les États-Unis ou les autorités sanitaires (FeMA) pour la FDA (États-Unis) pour le produit pharmaceutique) pour une autorisation d'utiliser l'a approuvé pour la commercialisation ou pour la mise sur l'ensemble de la population ou des patients pour la reproduction des patients.\n",
      "- Le dioxyde de titane est l'un des photocatalyseurs les plus répandus et le plus étudié en raison de sa stabilité, de sa toxicité faible et de ses propriétés optiques uniques. Il peut être utilisé pour des applications telles que la dégradation des polluants, la génération d'hydrogène et l'aménagement des sols. Graphite de carbone nitrure Le graphite d'azote nitrure est un photocatalyse à base de carbure organique, qui présente des propriétés catalytiques intéressantes pour la dégradation de polluants et de dégradation des métaux. Ces exemples de matériaux peuvent être utilisés en fonction des applications spécifiques, mais il existe de nombreux autres matériaux qui ne sont pas les plus utilisés, tels que le titane ou le ZnO, utilisés en majorité pour les métaux, qui présentent des différences de leurs propriétés optiques, comme le nickel ou les nitrate de titane ou les oxydes de fer fer ferro-solide de zinc.\n",
      "\n",
      "Question à traiter : Comment la photocatalyse rédox est actuellement appliquée en \"drug discovery\" du secteur pharmaceutique ?\n"
     ]
    }
   ],
   "source": [
    "# === TEST RAPIDE NO GUI ===\n",
    "question = 'Comment la photocatalyse rédox est actuellement appliquée en \"drug discovery\" du secteur pharmaceutique ?'\n",
    "context = get_relevant_context(question)\n",
    "prompt = generate_prompt_paragraph(context, question)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c4535a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === INTERFACE TKINTER ===\n",
    "\n",
    "def open_github(event):\n",
    "    webbrowser.open_new(\"https://github.com/victorcarre6\")\n",
    "\n",
    "def show_help():\n",
    "    help_text = (\n",
    "        \"LLM Memorization and Prompt Enhancer — Aide\\n\\n\"\n",
    "        \"• Synchroniser les conversations : ajoute les nouveaux échanges depuis LM Studio à la base de données.\\n\\n\"\n",
    "        \"• Générer prompt : extrait les mots-clés de votre question, cherche des conversations similaires dans votre base SQL, puis compresse les informations avec un LLM local.\\n\\n\"\n",
    "        \"Le prompt final est affiché puis automatiquement copié dans votre presse-papier !\\n\\n\"\n",
    "        \"Pour en savoir plus, obtenir plus d'informations à propos d'un éventuel bloquage des scripts, ou me contacter, visitez : github.com/victorcarre6/llm-memorization\"\n",
    "    )\n",
    "    help_window = tk.Toplevel(root)\n",
    "    help_window.title(\"Aide\")\n",
    "    help_window.geometry(\"500x300\")\n",
    "    help_window.configure(bg=\"#323232\")\n",
    "\n",
    "    text_widget = tk.Text(help_window, wrap=tk.WORD, font=(\"Segoe UI\", 8))\n",
    "    text_widget.insert(tk.END, help_text)\n",
    "    text_widget.config(state=tk.DISABLED)\n",
    "    text_widget.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    ok_button = tk.Button(help_window, text=\"Fermer\", command=help_window.destroy)\n",
    "    ok_button.pack(pady=10)\n",
    "\n",
    "# === CONFIGURATION DE L'INTERFACE ===\n",
    "root = tk.Tk()\n",
    "root.title(\"LLM Memorization and Prompt Enhancer\")\n",
    "root.geometry(\"850x650\")  # Légèrement augmenté pour meilleure disposition\n",
    "root.configure(bg=\"#323232\")\n",
    "\n",
    "# Style\n",
    "style = ttk.Style(root)\n",
    "style.theme_use('clam')\n",
    "\n",
    "# Configuration des styles\n",
    "style_config = {\n",
    "    'Green.TButton': {\n",
    "        'background': '#599258',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 11),\n",
    "        'padding': 6\n",
    "    },\n",
    "    'TLabel': {\n",
    "        'background': '#323232',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TEntry': {\n",
    "        'fieldbackground': '#FDF6EE',\n",
    "        'foreground': 'black',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TFrame': {'background': '#323232'},\n",
    "    'Status.TLabel': {\n",
    "        'background': '#323232',\n",
    "        'font': ('Segoe UI', 11)\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "for style_name, app_config in style_config.items():\n",
    "    style.configure(style_name, **app_config)\n",
    "\n",
    "style.map('Green.TButton',\n",
    "          background=[('active', '#457a3a'), ('pressed', '#2e4a20')],\n",
    "          foreground=[('disabled', '#d9d9d9')])\n",
    "\n",
    "# Widgets principaux\n",
    "main_frame = ttk.Frame(root, style='TFrame')\n",
    "main_frame.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Section question\n",
    "ttk.Label(main_frame, text=\"Poser la question :\").pack(pady=(0, 5))\n",
    "entry_question = ttk.Entry(main_frame, width=80, style='TEntry')\n",
    "entry_question.pack(pady=(0, 10))\n",
    "entry_question.bind(\"<Return>\", lambda event: on_ask())\n",
    "\n",
    "# Boutons\n",
    "button_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "button_frame.pack(pady=(0, 10))\n",
    "\n",
    "sync_button = ttk.Button(button_frame, text=\"Synchroniser les conversations\", \n",
    "                        command=sync_conversations, style='Green.TButton')\n",
    "sync_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "btn_ask = ttk.Button(button_frame, text=\"Générer prompt\", command=on_ask, style='Green.TButton')\n",
    "btn_ask.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Zone de sortie\n",
    "output_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "output_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "text_output = scrolledtext.ScrolledText(\n",
    "    output_frame, \n",
    "    width=100, \n",
    "    height=18, \n",
    "    font=('Segoe UI', 11), \n",
    "    wrap=tk.WORD, \n",
    "    bg=\"#FDF6EE\", \n",
    "    fg=\"black\", \n",
    "    insertbackground=\"black\"\n",
    ")\n",
    "text_output.pack(fill=tk.BOTH, expand=True, pady=(0, 5))\n",
    "\n",
    "# Barre de statut améliorée\n",
    "status_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "status_frame.pack(fill=tk.X, pady=(0, 5))\n",
    "\n",
    "label_status = ttk.Label(\n",
    "    status_frame, \n",
    "    text=\"Prêt\", \n",
    "    style='Status.TLabel',\n",
    "    foreground='white',\n",
    "    anchor='center',\n",
    "    justify='center',\n",
    "    wraplength=650  # Permet le retour à la ligne automatique\n",
    ")\n",
    "label_status.pack(side=tk.LEFT)\n",
    "\n",
    "# Pied de page\n",
    "footer_frame = ttk.Frame(root, style='TFrame')\n",
    "footer_frame.pack(side=tk.BOTTOM, fill=tk.X, padx=10, pady=(0, 5))\n",
    "\n",
    "footer = tk.Label(\n",
    "    footer_frame,\n",
    "    text=\"Développé par Victor Carré — GitHub\",\n",
    "    font=(\"Segoe UI\", 8, \"italic\"),\n",
    "    fg=\"white\",\n",
    "    bg=\"#323232\",\n",
    "    cursor=\"hand1\",\n",
    "    anchor=\"w\"\n",
    ")\n",
    "footer.pack(side=tk.LEFT, fill=tk.X, expand=True)\n",
    "footer.bind(\"<Button-1>\", open_github)\n",
    "\n",
    "help_button = ttk.Button(\n",
    "    footer_frame,\n",
    "    text=\"?\",\n",
    "    command=show_help,\n",
    "    width=2,\n",
    "    style='Green.TButton'\n",
    ")\n",
    "help_button.pack(side=tk.RIGHT)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa4815",
   "metadata": {},
   "source": [
    "# Idées\n",
    "\n",
    "- Mettre le choix du modèle dans le fichier `config.json`\n",
    "- Remplacer le hashage par du MD5\n",
    "- Colorer les contextes en fonction du nombre de keywords communs\n",
    "- Affichage d'un nuage de mots dans une seconde fenêtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d81295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"La photocatalyse est une méthode qui permet d'activer des réactions chimiques via la lumière. Elle est utilisée notamment dans l'industrie pharmaceutique pour faire des économies d'énergie et d'argent, selon une étude de la revue scientifique Plos Medecine, citée par le quotidien américain The Lancet dans son rapport annuel publié mardi.Le site de l'institut.\"}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "test = \"La photocatalyse est une méthode qui permet d'activer des réactions chimiques via la lumière. Elle est utilisée dans l'industrie pharmaceutique pour...\"\n",
    "output = summarizer(test, max_length=150, min_length=75)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d639a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
