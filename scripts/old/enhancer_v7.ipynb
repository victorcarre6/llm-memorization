{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67d9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import pyperclip\n",
    "import tkinter as tk\n",
    "import subprocess\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from tkinter import scrolledtext, ttk\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from keybert import KeyBERT\n",
    "import os\n",
    "import webbrowser\n",
    "import spacy\n",
    "import torch\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "886247f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# === INITIALISATION ===\n",
    "\n",
    "# Chemin absolu vers le dossier racine du projet\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "config_path = os.path.join(PROJECT_ROOT, \"config.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    raw_config = json.load(f)\n",
    "\n",
    "# Chargement de la config\n",
    "config = {}\n",
    "for key, value in raw_config.items():\n",
    "    if isinstance(value, str):\n",
    "        expanded = os.path.expanduser(value)\n",
    "        if not os.path.isabs(expanded):\n",
    "            expanded = os.path.normpath(os.path.join(PROJECT_ROOT, expanded))\n",
    "        config[key] = expanded\n",
    "    else:\n",
    "        config[key] = value\n",
    "stopwords_path = config.get(\"stopwords_file_path\", \"stopwords_fr.json\")\n",
    "with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    french_stop_words = set(json.load(f))\n",
    "\n",
    "# Constantes\n",
    "TOP_K = 5\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "summarizing_model = \"moussaKam/barthez-orangesum-abstract\"\n",
    "\n",
    "# Connexion à la base SQLite\n",
    "conn = sqlite3.connect(config[\"db_path\"])\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Initialisation des modèles\n",
    "kw_model = KeyBERT()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(summarizing_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(summarizing_model)\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7be31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FONCTIONS PRINCIPALES ===\n",
    "\n",
    "# Go\n",
    "\n",
    "def on_ask():\n",
    "    question = entry_question.get()\n",
    "    if not question.strip():\n",
    "        update_status(\"⚠️ Merci de saisir une question.\", error=True)\n",
    "        return\n",
    "    \n",
    "    update_status(\"⌛ Traitement en cours...\")\n",
    "    root.update()\n",
    "\n",
    "    try:\n",
    "        context = get_relevant_context(question)\n",
    "        prompt = generate_prompt_paragraph(context, question)\n",
    "        pyperclip.copy(prompt)\n",
    "\n",
    "        text_output.delete('1.0', tk.END)\n",
    "        text_output.insert(tk.END, prompt)\n",
    "        \n",
    "        # Calcul des métriques\n",
    "        context_count = len(context)\n",
    "        token_count = len(prompt.split())  # Estimation simple - remplacer par tokenizer réel si nécessaire\n",
    "        \n",
    "        update_status(\n",
    "            f\"Prompt généré ({token_count} tokens) | Contexte utilisé : {context_count} éléments\",\n",
    "            success=True\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        update_status(f\"Erreur : {str(e)}\", error=True)\n",
    "\n",
    "def update_status(message, error=False, success=False):\n",
    "    \"\"\"Met à jour le label de statut avec style approprié\"\"\"\n",
    "    label_status.config(text=message)\n",
    "    if error:\n",
    "        label_status.config(foreground='#ff6b6b')\n",
    "    elif success:\n",
    "        label_status.config(foreground='#599258')\n",
    "    else:\n",
    "        label_status.config(foreground='white')\n",
    "\n",
    "# Synchronisation des conversations\n",
    "def sync_conversations():\n",
    "    try:\n",
    "        global config\n",
    "        sync_path = config.get(\"sync_script_path\")\n",
    "        if not sync_path:\n",
    "            label_status.config(text=\"sync_script_path introuvable.\")\n",
    "            return\n",
    "\n",
    "        subprocess.run([\"python3\", config[\"sync_script_path\"]], check=True)\n",
    "        label_status.config(text=\"Synchronisation terminée.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        label_status.config(text=\"Erreur lors de la synchronisation.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1054a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RÉCUPÉRATION DU CONTEXTE ===\n",
    "\n",
    "# Récupération des mots-clés de la question initiale\n",
    "def extract_keywords(text, top_n=20):\n",
    "    raw_keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 1),  # mots uniques uniquement\n",
    "        stop_words='english',\n",
    "        top_n=top_n * 2  # extraire plus pour filtrer ensuite\n",
    "    )\n",
    "\n",
    "    seen = set()\n",
    "    filtered_keywords = []\n",
    "\n",
    "    for kw, _ in raw_keywords:\n",
    "        kw_clean = kw.lower().strip()\n",
    "        # filtre : pas de stopwords, que des mots alphabétiques, min 3 lettres, pas de doublons\n",
    "        if (\n",
    "            kw_clean not in seen and\n",
    "            kw_clean not in french_stop_words and\n",
    "            len(kw_clean) > 2 and\n",
    "            re.match(r'^[a-zA-Z\\-]+$', kw_clean)\n",
    "        ):\n",
    "            seen.add(kw_clean)\n",
    "            filtered_keywords.append(kw_clean)\n",
    "\n",
    "        if len(filtered_keywords) >= top_n:\n",
    "            break\n",
    "\n",
    "    return filtered_keywords\n",
    "\n",
    "# Requête SQL en fonction des mots-clés extraits\n",
    "def get_relevant_context(user_question, limit=TOP_K):\n",
    "    keywords = extract_keywords(user_question)\n",
    "    print(f\"Mots-clés extraits de la question : {keywords}\")\n",
    "    if not keywords:\n",
    "        return []\n",
    "\n",
    "    # Récupérer toutes les conversations contenant au moins un mot-clé\n",
    "    placeholders = ', '.join(['?'] * len(keywords))\n",
    "    query = f'''\n",
    "        SELECT c.id, c.user_input, c.llm_output, c.timestamp, k.keyword\n",
    "        FROM conversations c\n",
    "        JOIN keywords k ON c.id = k.conversation_id\n",
    "        WHERE k.keyword IN ({placeholders})\n",
    "    '''\n",
    "    cur.execute(query, (*keywords,))\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Compter les mots-clés en commun par conversation\n",
    "    match_counts = {}\n",
    "    context_data = {}\n",
    "\n",
    "    for convo_id, user_input, llm_output, timestamp, keyword in rows:\n",
    "        if convo_id not in match_counts:\n",
    "            match_counts[convo_id] = set()\n",
    "            context_data[convo_id] = (user_input, llm_output, timestamp)\n",
    "        match_counts[convo_id].add(keyword)\n",
    "\n",
    "    # Calcul du score = nombre de mots-clés en commun\n",
    "    scored_contexts = [\n",
    "        (convo_id, len(keywords_matched))\n",
    "        for convo_id, keywords_matched in match_counts.items()\n",
    "    ]\n",
    "\n",
    "    # Tri décroissant selon le nombre de mots-clés en commun\n",
    "    sorted_contexts = sorted(scored_contexts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Récupération des résultats top-N\n",
    "    filtered_context = []\n",
    "    for convo_id, score in sorted_contexts[:limit]:\n",
    "        filtered_context.append(context_data[convo_id])\n",
    "\n",
    "    return filtered_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104bd9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FONCTION UTILITAIRE NLP  ===\n",
    "\n",
    "def nlp_clean_text(text, max_chunk_size=500):\n",
    "    # Suppression des blocs de code\n",
    "    text = re.sub(r'```(?:python)?\\s*.*?```', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Analyse NLP\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # On découpe par phrases tout en nettoyant le style\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        s = sent.text.strip()\n",
    "        if len(s) < 20:\n",
    "            continue  # ignore phrases trop courtes\n",
    "\n",
    "        if current_length + len(s) < max_chunk_size:\n",
    "            current_chunk.append(s)\n",
    "            current_length += len(s)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [s]\n",
    "            current_length = len(s)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return \" \".join(chunks[:3])  # limite à 3 blocs maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "665f5df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONSTRUCTION DU PROMPT ===\n",
    "\n",
    "# Compression primaire du contexte extrait\n",
    "def summarize(text, focus_terms=None, max_length=1024):\n",
    "    try:\n",
    "        print(\"[1] Texte original :\", text[:200], \"...\")\n",
    "        # Pré-filtrage des phrases importantes si focus_terms donné\n",
    "        if focus_terms:\n",
    "            sentences = [s for s in text.split('.') \n",
    "                        if any(term.lower() in s.lower() for term in focus_terms)]\n",
    "            text = '. '.join(sentences)[:2000] or text[:2000]\n",
    "            print(\"[2] Texte après focus_terms :\", text[:200], \"...\")\n",
    "\n",
    "        # Nettoyage : garder seulement les mots non-stopwords\n",
    "        # On convertit en minuscules, puis on filtre mots\n",
    "        #import re\n",
    "        #words = re.findall(r\"\\b\\w+\\b\", text.lower())  # Extraire mots uniquement\n",
    "        #filtered_words = [w for w in words if w not in french_stop_words]\n",
    "\n",
    "        # Reconstituer le texte filtré\n",
    "        #filtered_text = ' '.join(filtered_words)\n",
    "        #print(\"[3] Texte filtré :\", filtered_text[:200], \"...\")\n",
    "        \n",
    "        # Résumé avec le texte filtré\n",
    "        print(\"[4] Appel au summarizer (taille texte : {})\".format(len(text)))\n",
    "        result = summarizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            min_length=max_length // 2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        print(\"[5] Résultat brut :\", result)\n",
    "        return nlp_clean_text(result[0]['summary_text'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur summarization : {e}\")\n",
    "        return text[:max_length] + \"... [résumé tronqué]\"\n",
    "\n",
    "\n",
    "\n",
    "# Construction du prompt\n",
    "\n",
    "def generate_prompt_paragraph(context, question, target_tokens=1000):\n",
    "    question_keywords = extract_keywords(question, top_n=5)\n",
    "    if not context:\n",
    "        return f\"Voici une nouvelle question à traiter : {question}\"\n",
    "\n",
    "    # 1. Prétraitement intelligent du contexte\n",
    "    processed_items = []\n",
    "    for item in context[:3]:  # Nombre max d'éléments dans le contexte\n",
    "        try:\n",
    "            # Extraction sécurisée\n",
    "            user_input = str(item[0])[:300]  # Troncature des questions longues\n",
    "            llm_output = str(item[1])\n",
    "            keyword = str(item[5]) if len(item) > 5 and str(item[3]).strip() not in {\"\", \"none\", \"null\", \"1\", \"2\", \"3\"} else None\n",
    "\n",
    "            # Summarization avec gestion de la longueur\n",
    "            summary = summarize(\n",
    "                text=llm_output,\n",
    "                # focus_terms=question_keywords,  # tu peux réactiver si tu veux\n",
    "                # max_length=512\n",
    "            )\n",
    "\n",
    "            # Nettoyage et segmentation du texte via nlp_clean_text\n",
    "            print(f\"Avant segmentation : {len(summary.split())} mots\")\n",
    "            cleaned_summary = nlp_clean_text(summary)\n",
    "            processed_items.append({\n",
    "                'question': user_input,\n",
    "                'summary': cleaned_summary,\n",
    "                'keyword': keyword.lower().strip() if keyword else None\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur traitement item : {e}\")\n",
    "            continue\n",
    "\n",
    "    # 2. Construction du prompt\n",
    "    parts = []\n",
    "\n",
    "    # Partie questions\n",
    "    if processed_items:\n",
    "        questions = [f\"'{item['question']}'\" for item in processed_items]\n",
    "        if len(questions) == 1:\n",
    "            parts.append(f\"Question précédente : {questions[0]}\")\n",
    "        else:\n",
    "            *init, last = questions\n",
    "            parts.append(f\"Questions antérieures : {', '.join(init)}, et enfin {last}\")\n",
    "\n",
    "    # Partie mots-clés\n",
    "    keywords = {item['keyword'] for item in processed_items if item['keyword']}\n",
    "    if keywords:\n",
    "        parts.append(f\"Mots-clés pertinents : {', '.join(sorted(keywords))}\")\n",
    "\n",
    "    # Partie résumés\n",
    "    if processed_items:\n",
    "        summaries = [f\"- {item['summary']}\" for item in processed_items]\n",
    "        parts.append(\"Contexte pertinent :\\n\" + \"\\n\".join(summaries))\n",
    "\n",
    "    # Question actuelle\n",
    "    parts.append(f\"\\nQuestion à traiter : {question}\")\n",
    "\n",
    "    return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c5769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FONCTIONS UTILITAIRES MANUELLES ===\n",
    "\n",
    "def clean_and_segment(text, max_chunk_size=500):\n",
    "    # Suppression des blocs de code (```python ... ``` et ``` ...)\n",
    "    text = re.sub(r'```(?:python)?\\s*.*?```', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Mise en minuscules + remplacement de mots\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"and\", \"et\")\n",
    "    text = text.replace(\":\", \",\")\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    \n",
    "    # Séparation en phrases pour filtrage\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 20 and not re.search(r'[^a-zA-Z0-9À-ÿ\\s\\-\\(\\),.:;\\'\"]', s)]\n",
    "    \n",
    "    # Recomposition du texte nettoyé\n",
    "    cleaned_text = ' '.join(sentences)\n",
    "    \n",
    "    # Suppression des répétitions de mots consécutifs\n",
    "    cleaned_text = re.sub(r'(\\b\\w+\\b)(\\s+\\1\\b)+', r'\\1', cleaned_text)\n",
    "    \n",
    "    # Ponctuation finale\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    if not cleaned_text.endswith((\".\", \"!\", \"?\")):\n",
    "        cleaned_text += \".\"\n",
    "    \n",
    "    # Capitalisation initiale\n",
    "    cleaned_text = cleaned_text.capitalize()\n",
    "    \n",
    "    # --- Segmentation ---\n",
    "    # Nettoyage final léger (espaces)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    # Découpage par phrases techniques (par points)\n",
    "    segments = [s.strip() for s in cleaned_text.split('.') if s.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        length_current = sum(len(s) for s in current_chunk)\n",
    "        if length_current + len(segment) < max_chunk_size:\n",
    "            current_chunk.append(segment)\n",
    "        else:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            current_chunk = [segment]\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "    \n",
    "    # Limite à 3 chunks max, puis recomposition en un seul texte segmenté\n",
    "    return ' '.join(chunks[:3])\n",
    "\n",
    "\n",
    "\n",
    "# Correction des erreurs de français courantes\n",
    "def clean_french(text):\n",
    "    \"\"\"Corrige les erreurs de français courantes\"\"\"\n",
    "    replacements = [\n",
    "        (\"le processus\", \"la procédure\"),\n",
    "        (\"testing\", \"test\"),\n",
    "        (\"identifying\", \"identification\"),\n",
    "        (\"synthésising\", \"synthèse\"),\n",
    "        (\"drug discovery\", \"découverte de médicaments\")\n",
    "    ]\n",
    "    for old, new in replacements:\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62681f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 492. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=246)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots-clés extraits de la question : ['photocatalyse', 'pharmaceutique', 'drug', 'discovery', 'actuellement', 'secteur']\n",
      "[1] Texte original : Le processus de découverte de médicaments, ou \"drug discovery\", est un processus complexe et coûteux impliquant plusieurs étapes pour identifier de nouveaux médicaments potentiels. Voici un aperçu des ...\n",
      "[4] Appel au summarizer (taille texte : 2782)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "/Users/victorcarre/venvs/Jupyter/lib/python3.13/site-packages/transformers/generation/utils.py:1633: UserWarning: Unfeasible length constraints: `min_length` (512) is larger than the maximum possible length (257). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1024, but your input_length is only 533. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=266)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=1024) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Résultat brut : [{'summary_text': \"Le processus de drug discovery est un des plus importants et coûteux dans le secteur pharmaceutique. Voici les étapes clés du processus : 1. Identification des cibles : La première étape consiste à identifier les protéines ou les mécanismes cellulaires impliqués dans une maladie donnée ; ensuite, les scientifiques développent des molécules qui interagissent avec la cible et modulent son activité (chimiques, biologiques, cytogènes, etc). 3. Synthèse de composés : Les molécules conçues sont then synthétisées et testées en laboratoire pour déterminer leur efficacité et leur sécurité. 7. Approbation réglementaire : Si un médicament réussit ses essais cliniques, l'entreprise qui le développe doit soumettre une demande de ses filiales à une agence réglementaire, telle que la FDA en Europe ou l'Agence européenne de l'assurance maladie (Agence de la santé) en Europe (Agence des médicaments ou des produits de la sécurité du médicament) a une autorisation de la FDA pour l'agence du médicament (ANSA) pour les États-Unis ou les autorités sanitaires (FeMA) pour la FDA (États-Unis) pour le produit pharmaceutique) pour une autorisation d'utiliser l'a approuvé pour la commercialisation ou pour la mise sur l'ensemble de la population ou des patients pour la reproduction des patients. C'est le cas :\"}]\n",
      "Avant segmentation : 193 mots\n",
      "[1] Texte original : Les photocatalyseurs rédox sont largement utilisés dans diverses applications industrielles pour catalyser des réactions chimiques à travers le transfert d'électrons induit par la lumière. Voici quelq ...\n",
      "[4] Appel au summarizer (taille texte : 2447)\n",
      "[5] Résultat brut : [{'summary_text': \"Le dioxyde de titane est l'un des photocatalyseurs les plus répandus et le plus étudié en raison de sa stabilité, de sa toxicité faible et de ses propriétés optiques uniques. Il peut être utilisé pour des applications telles que la dégradation des polluants, la génération d'hydrogène et l'aménagement des sols. 6. g-C3N4 (Graphite de carbone nitrure) : Le graphite d'azote nitrure est un photocatalyse à base de carbure organique, qui présente des propriétés catalytiques intéressantes pour la dégradation de polluants et de dégradation des métaux. Ces exemples de matériaux peuvent être utilisés en fonction des applications spécifiques, mais il existe de nombreux autres matériaux qui ne sont pas les plus utilisés, tels que le titane ou le ZnO, utilisés en majorité pour les métaux, qui présentent des différences de leurs propriétés optiques, comme le nickel ou les nitrate de titane ou les oxydes de fer fer ferro-solide de zinc.L'Oxide de zinc ou le chrome, utilisés dans la composition des métaux, utilisés pour la composition de leur composition ou les métaux communs, utilisés par exemple, pour la photoélectrochimie.Figure de cadmium ou les germaniques, pour l'optimisation des flux d'\"}]\n",
      "Avant segmentation : 146 mots\n",
      "Questions antérieures : 'Comment faire du \"drug discovery\" dans le secteur pharmaceutique ?', et enfin 'Quels sont les photocatalyseurs rédox les plus utilisés dans l'industrie ?'\n",
      "Contexte pertinent :\n",
      "- Le processus de drug discovery est un des plus importants et coûteux dans le secteur pharmaceutique. Voici les étapes clés du processus : 1. Identification des cibles : La première étape consiste à identifier les protéines ou les mécanismes cellulaires impliqués dans une maladie donnée ; ensuite, les scientifiques développent des molécules qui interagissent avec la cible et modulent son activité (chimiques, biologiques, cytogènes, etc). Synthèse de composés : Les molécules conçues sont then synthétisées et testées en laboratoire pour déterminer leur efficacité et leur sécurité. Approbation réglementaire : Si un médicament réussit ses essais cliniques, l'entreprise qui le développe doit soumettre une demande de ses filiales à une agence réglementaire, telle que la FDA en Europe ou l'Agence européenne de l'assurance maladie (Agence de la santé) en Europe (Agence des médicaments ou des produits de la sécurité du médicament) a une autorisation de la FDA pour l'agence du médicament (ANSA) pour les États-Unis ou les autorités sanitaires (FeMA) pour la FDA (États-Unis) pour le produit pharmaceutique) pour une autorisation d'utiliser l'a approuvé pour la commercialisation ou pour la mise sur l'ensemble de la population ou des patients pour la reproduction des patients.\n",
      "- Le dioxyde de titane est l'un des photocatalyseurs les plus répandus et le plus étudié en raison de sa stabilité, de sa toxicité faible et de ses propriétés optiques uniques. Il peut être utilisé pour des applications telles que la dégradation des polluants, la génération d'hydrogène et l'aménagement des sols. Graphite de carbone nitrure Le graphite d'azote nitrure est un photocatalyse à base de carbure organique, qui présente des propriétés catalytiques intéressantes pour la dégradation de polluants et de dégradation des métaux. Ces exemples de matériaux peuvent être utilisés en fonction des applications spécifiques, mais il existe de nombreux autres matériaux qui ne sont pas les plus utilisés, tels que le titane ou le ZnO, utilisés en majorité pour les métaux, qui présentent des différences de leurs propriétés optiques, comme le nickel ou les nitrate de titane ou les oxydes de fer fer ferro-solide de zinc.\n",
      "\n",
      "Question à traiter : Comment la photocatalyse rédox est actuellement appliquée en \"drug discovery\" du secteur pharmaceutique ?\n"
     ]
    }
   ],
   "source": [
    "# === TEST RAPIDE NO GUI ===\n",
    "question = 'Comment la photocatalyse rédox est actuellement appliquée en \"drug discovery\" du secteur pharmaceutique ?'\n",
    "context = get_relevant_context(question)\n",
    "prompt = generate_prompt_paragraph(context, question)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c4535a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# === INTERFACE TKINTER ===\n",
    "\n",
    "def open_github(event):\n",
    "    webbrowser.open_new(\"https://github.com/victorcarre6\")\n",
    "\n",
    "def show_help():\n",
    "    help_text = (\n",
    "        \"LLM Memorization and Prompt Enhancer — Aide\\n\\n\"\n",
    "        \"• Synchroniser les conversations : ajoute les nouveaux échanges depuis LM Studio à la base de données.\\n\\n\"\n",
    "        \"• Générer prompt : extrait les mots-clés de votre question, cherche des conversations similaires dans votre base SQL, puis compresse les informations avec un LLM local.\\n\\n\"\n",
    "        \"Le prompt final est affiché puis automatiquement copié dans votre presse-papier !\\n\\n\"\n",
    "        \"Pour en savoir plus, obtenir plus d'informations à propos d'un éventuel bloquage des scripts, ou me contacter, visitez : github.com/victorcarre6/llm-memorization\"\n",
    "    )\n",
    "    help_window = tk.Toplevel(root)\n",
    "    help_window.title(\"Aide\")\n",
    "    help_window.geometry(\"500x300\")\n",
    "    help_window.configure(bg=\"#323232\")\n",
    "\n",
    "    text_widget = tk.Text(help_window, wrap=tk.WORD, font=(\"Segoe UI\", 8))\n",
    "    text_widget.insert(tk.END, help_text)\n",
    "    text_widget.config(state=tk.DISABLED)\n",
    "    text_widget.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "    ok_button = tk.Button(help_window, text=\"Fermer\", command=help_window.destroy)\n",
    "    ok_button.pack(pady=10)\n",
    "\n",
    "# === CONFIGURATION DE L'INTERFACE ===\n",
    "root = tk.Tk()\n",
    "root.title(\"LLM Memorization and Prompt Enhancer\")\n",
    "root.geometry(\"850x650\")  # Légèrement augmenté pour meilleure disposition\n",
    "root.configure(bg=\"#323232\")\n",
    "\n",
    "# Style\n",
    "style = ttk.Style(root)\n",
    "style.theme_use('clam')\n",
    "\n",
    "# Configuration des styles\n",
    "style_config = {\n",
    "    'Green.TButton': {\n",
    "        'background': '#599258',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 11),\n",
    "        'padding': 6\n",
    "    },\n",
    "    'TLabel': {\n",
    "        'background': '#323232',\n",
    "        'foreground': 'white',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TEntry': {\n",
    "        'fieldbackground': '#FDF6EE',\n",
    "        'foreground': 'black',\n",
    "        'font': ('Segoe UI', 11)\n",
    "    },\n",
    "    'TFrame': {'background': '#323232'},\n",
    "    'Status.TLabel': {\n",
    "        'background': '#323232',\n",
    "        'font': ('Segoe UI', 11)\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "for style_name, app_config in style_config.items():\n",
    "    style.configure(style_name, **app_config)\n",
    "\n",
    "style.map('Green.TButton',\n",
    "          background=[('active', '#457a3a'), ('pressed', '#2e4a20')],\n",
    "          foreground=[('disabled', '#d9d9d9')])\n",
    "\n",
    "# Widgets principaux\n",
    "main_frame = ttk.Frame(root, style='TFrame')\n",
    "main_frame.pack(padx=10, pady=10, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Section question\n",
    "ttk.Label(main_frame, text=\"Poser la question :\").pack(pady=(0, 5))\n",
    "entry_question = ttk.Entry(main_frame, width=80, style='TEntry')\n",
    "entry_question.pack(pady=(0, 10))\n",
    "entry_question.bind(\"<Return>\", lambda event: on_ask())\n",
    "\n",
    "# Boutons\n",
    "button_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "button_frame.pack(pady=(0, 10))\n",
    "\n",
    "sync_button = ttk.Button(button_frame, text=\"Synchroniser les conversations\", \n",
    "                        command=sync_conversations, style='Green.TButton')\n",
    "sync_button.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "btn_ask = ttk.Button(button_frame, text=\"Générer prompt\", command=on_ask, style='Green.TButton')\n",
    "btn_ask.pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Zone de sortie\n",
    "output_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "output_frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "text_output = scrolledtext.ScrolledText(\n",
    "    output_frame, \n",
    "    width=100, \n",
    "    height=18, \n",
    "    font=('Segoe UI', 11), \n",
    "    wrap=tk.WORD, \n",
    "    bg=\"#FDF6EE\", \n",
    "    fg=\"black\", \n",
    "    insertbackground=\"black\"\n",
    ")\n",
    "text_output.pack(fill=tk.BOTH, expand=True, pady=(0, 5))\n",
    "\n",
    "# Barre de statut améliorée\n",
    "status_frame = ttk.Frame(main_frame, style='TFrame')\n",
    "status_frame.pack(fill=tk.X, pady=(0, 5))\n",
    "\n",
    "label_status = ttk.Label(\n",
    "    status_frame, \n",
    "    text=\"Prêt\", \n",
    "    style='Status.TLabel',\n",
    "    foreground='white',\n",
    "    anchor='center',\n",
    "    justify='center',\n",
    "    wraplength=650  # Permet le retour à la ligne automatique\n",
    ")\n",
    "label_status.pack(side=tk.LEFT)\n",
    "\n",
    "# Pied de page\n",
    "footer_frame = ttk.Frame(root, style='TFrame')\n",
    "footer_frame.pack(side=tk.BOTTOM, fill=tk.X, padx=10, pady=(0, 5))\n",
    "\n",
    "footer = tk.Label(\n",
    "    footer_frame,\n",
    "    text=\"Développé par Victor Carré — GitHub\",\n",
    "    font=(\"Segoe UI\", 8, \"italic\"),\n",
    "    fg=\"white\",\n",
    "    bg=\"#323232\",\n",
    "    cursor=\"hand1\",\n",
    "    anchor=\"w\"\n",
    ")\n",
    "footer.pack(side=tk.LEFT, fill=tk.X, expand=True)\n",
    "footer.bind(\"<Button-1>\", open_github)\n",
    "\n",
    "help_button = ttk.Button(\n",
    "    footer_frame,\n",
    "    text=\"?\",\n",
    "    command=show_help,\n",
    "    width=2,\n",
    "    style='Green.TButton'\n",
    ")\n",
    "help_button.pack(side=tk.RIGHT)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa4815",
   "metadata": {},
   "source": [
    "# Idées\n",
    "\n",
    "- Mettre le choix du modèle dans le fichier `config.json`\n",
    "- Remplacer le hashage par du MD5\n",
    "- Colorer les contextes en fonction du nombre de keywords communs\n",
    "- Affichage d'un nuage de mots dans une seconde fenêtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d81295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \"La photocatalyse est une méthode qui permet d'activer des réactions chimiques via la lumière. Elle est utilisée notamment dans l'industrie pharmaceutique pour faire des économies d'énergie et d'argent, selon une étude de la revue scientifique Plos Medecine, citée par le quotidien américain The Lancet dans son rapport annuel publié mardi.Le site de l'institut.\"}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "test = \"La photocatalyse est une méthode qui permet d'activer des réactions chimiques via la lumière. Elle est utilisée dans l'industrie pharmaceutique pour...\"\n",
    "output = summarizer(test, max_length=150, min_length=75)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d639a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChemML (jupyter_venv)",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
