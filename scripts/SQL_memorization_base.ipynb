{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ac9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/victorcarre/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports de base\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from keybert import KeyBERT\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialisation\n",
    "db_path = 'conversations.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Pour usage SQLAlchemy plus tard\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "# Initialisation KeyBERT\n",
    "kw_model = KeyBERT()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31005ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des tables\n",
    "cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS conversations (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        user_input TEXT,\n",
    "        llm_output TEXT,\n",
    "        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "''')\n",
    "\n",
    "cur.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS keywords (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        conversation_id INTEGER,\n",
    "        keyword TEXT,\n",
    "        FOREIGN KEY(conversation_id) REFERENCES conversations(id)\n",
    "    )\n",
    "''')\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "374e0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_conversation(user_input, llm_output):\n",
    "    \"\"\"\n",
    "    Insère une nouvelle conversation + mots-clés associés dans la base de données.\n",
    "    \"\"\"\n",
    "    combined_text = user_input + \" \" + llm_output\n",
    "    keywords = extract_keywords(combined_text)\n",
    "\n",
    "    cur.execute(\"INSERT INTO conversations (user_input, llm_output) VALUES (?, ?)\", (user_input, llm_output))\n",
    "    conversation_id = cur.lastrowid\n",
    "\n",
    "    for kw in keywords:\n",
    "        cur.execute(\"INSERT INTO keywords (conversation_id, keyword) VALUES (?, ?)\", (conversation_id, kw))\n",
    "    \n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d38af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Utilise KeyBERT pour extraire des mots-clés pertinents.\n",
    "    \"\"\"\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=top_n)\n",
    "    return [kw for kw, _ in keywords]\n",
    "\n",
    "def insert_conversation(user_input, llm_output):\n",
    "    \"\"\"\n",
    "    Insère une conversation dans la base et ses mots-clés associés.\n",
    "    \"\"\"\n",
    "    combined_text = f\"{user_input} {llm_output}\"\n",
    "    keywords = extract_keywords(combined_text)\n",
    "\n",
    "    cur.execute(\n",
    "        \"INSERT INTO conversations (user_input, llm_output) VALUES (?, ?)\",\n",
    "        (user_input, llm_output)\n",
    "    )\n",
    "    conversation_id = cur.lastrowid\n",
    "\n",
    "    for kw in keywords:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO keywords (conversation_id, keyword) VALUES (?, ?)\",\n",
    "            (conversation_id, kw)\n",
    "        )\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9ea04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_conversations_by_keyword(search_term):\n",
    "    \"\"\"\n",
    "    Recherche toutes les conversations associées à un mot-clé donné.\n",
    "    \"\"\"\n",
    "    cur.execute('''\n",
    "        SELECT c.user_input, c.llm_output, c.timestamp\n",
    "        FROM conversations c\n",
    "        JOIN keywords k ON c.id = k.conversation_id\n",
    "        WHERE k.keyword LIKE ?\n",
    "        ORDER BY c.timestamp DESC\n",
    "    ''', (f'%{search_term}%',))\n",
    "\n",
    "    return cur.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ff9cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Comment fonctionnent les panneaux solaires ?\n",
      "A: Les panneaux solaires transforment la lumière en électricité via l'effet photovoltaïque.\n",
      "⏱️ 2025-06-07 22:52:39\n",
      "--------------------------------------------------\n",
      "Q: Comment fonctionnent les panneaux solaires ?\n",
      "A: Les panneaux solaires transforment la lumière en électricité via l'effet photovoltaïque.\n",
      "⏱️ 2025-06-07 22:52:39\n",
      "--------------------------------------------------\n",
      "Q: Comment fonctionnent les panneaux solaires ?\n",
      "A: Les panneaux solaires transforment la lumière en électricité via l'effet photovoltaïque.\n",
      "⏱️ 2025-06-07 22:52:39\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Exemple : insertion\n",
    "user_input = \"Comment fonctionnent les panneaux solaires ?\"\n",
    "llm_output = \"Les panneaux solaires transforment la lumière en électricité via l'effet photovoltaïque.\"\n",
    "\n",
    "insert_conversation(user_input, llm_output)\n",
    "\n",
    "# Recherche\n",
    "results = search_conversations_by_keyword(\"solaire\")\n",
    "for r in results:\n",
    "    print(\"Q:\", r[0])\n",
    "    print(\"A:\", r[1])\n",
    "    print(\"⏱️\", r[2])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2576e09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "timestamp",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "user_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_output",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keywords",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "fb658264-6b47-432e-935d-938e2b5a529b",
       "rows": [
        [
         "0",
         "50",
         "2025-06-07 22:53:11",
         "(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Victor ~ % python -m pip install gensim\nCollecting gensim\n  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting numpy<2.0,>=1.18.5 (from gensim)\n  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n  Using cached scipy-1.13.1.tar.gz (57.2 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  \n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [45 lines of output]\n      + meson setup /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-en4cz4pc/scipy_ab2a417aba66481f94d7426bc39eb30d /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-en4cz4pc/scipy_ab2a417aba66481f94d7426bc39eb30d/.mesonpy-vnu6yzke -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-en4cz4pc/scipy_ab2a417aba66481f94d7426bc39eb30d/.mesonpy-vnu6yzke/meson-python-native-file.ini\n      The Meson build system\n      Version: 1.8.1\n      Source dir: /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-en4cz4pc/scipy_ab2a417aba66481f94d7426bc39eb30d\n      Build dir: /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-en4cz4pc/scipy_ab2a417aba66481f94d7426bc39eb30d/.mesonpy-vnu6yzke\n      Build type: native build\n      Project name: scipy\n      Project version: 1.13.1\n      C compiler for the host machine: cc (clang 17.0.0 \"Apple clang version 17.0.0 (clang-1700.0.13.3)\")\n      C linker for the host machine: cc ld64 1167.4.1\n      C++ compiler for the host machine: c++ (clang 17.0.0 \"Apple clang version 17.0.0 (clang-1700.0.13.3)\")\n      C++ linker for the host machine: c++ ld64 1167.4.1\n      Cython compiler for the host machine: cython (cython 3.0.12)\n      Host machine cpu family: aarch64\n      Host machine cpu: aarch64\n      Program python found: YES (/Users/victorcarre/venvs/Jupyter/bin/python)\n      Found pkg-config: YES (/opt/homebrew/bin/pkg-config) 2.4.3\n      Run-time dependency python found: YES 3.13\n      Program cython found: YES (/private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-build-env-_29mcl8o/overlay/bin/cython)\n      Compiler for C supports arguments -Wno-unused-but-set-variable: YES\n      Compiler for C supports arguments -Wno-unused-function: YES\n      Compiler for C supports arguments -Wno-conversion: YES\n      Compiler for C supports arguments -Wno-misleading-indentation: YES\n      Library m found: YES\n      Fortran compiler for the host machine: gfortran (gcc 15.1.0 \"GNU Fortran (Homebrew GCC 15.1.0) 15.1.0\")\n      Fortran linker for the host machine: gfortran ld64 1167.4.1\n      Compiler for Fortran supports arguments -Wno-conversion: YES\n      Compiler for C supports link arguments -Wl,-ld_classic: YES\n      Checking if \"-Wl,--version-script\" links: NO\n      Program pythran found: YES 0.15.0 0.15.0 (/private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-build-env-_29mcl8o/overlay/bin/pythran)\n      Did not find CMake 'cmake'\n      Found CMake: NO\n      Run-time dependency xsimd found: NO (tried pkgconfig, framework and cmake)\n      Run-time dependency threads found: YES\n      Library npymath found: YES\n      Library npyrandom found: YES\n      pybind11-config found: YES (/private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-build-env-_29mcl8o/overlay/bin/pybind11-config) 2.12.1\n      Run-time dependency pybind11 found: YES 2.12.1\n      Run-time dependency scipy-openblas found: NO (tried pkgconfig)\n      Run-time dependency openblas found: NO (tried pkgconfig, framework and cmake)\n      Run-time dependency openblas found: NO (tried pkgconfig and framework)\n      \n      ../scipy/meson.build:163:9: ERROR: Dependency \"OpenBLAS\" not found, tried pkgconfig and framework\n      \n      A full log can be found at /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-en4cz4pc/scipy_ab2a417aba66481f94d7426bc39eb30d/.mesonpy-vnu6yzke/meson-logs/meson-log.txt\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Victor ~ % brew install gfortran\nWarning: gcc 15.1.0 is already installed and up-to-date.\nTo reinstall 15.1.0, run:\n  brew reinstall gcc",
         "It seems like there's a problem with the OpenBLAS library when trying to install scipy via pip. The error message suggests that it can't find the OpenBLAS library. Here are some things you could try:\n\n1. Install GCC and BLAS/LAPACK libraries separately if they aren't already installed. \n```bash\nbrew install gcc\nbrew install openblas\n```\nThen use pip to install scipy, but explicitly tell it where the OpenBLAS library is located:\n```bash\npip install scipy --global-option=build_ext --global-option=\"-L/usr/local/opt/openblas/lib\" --global-option=\"-I/usr/local/opt/openblas/include\"\n```\n2. You might need to specify the path to your gcc:\n```bash\nCC=/usr/bin/gcc-10 pip install scipy\n```\n3. Try using conda (a package manager for Python and other languages). The Anaconda distribution includes the necessary libraries:\n```bash\nconda create -n myenv numpy scipy pandas jupyter matplotlib tensorflow keras\n```\n4. If all else fails, consider downgrading your gcc version:\n```bash\nbrew install gcc@9\n```\nAnd then use the previous pip command with this new compiler:\n```bash\nCC=/usr/bin/gcc-9 pip install scipy\n```\n\nRemember to replace \"/usr/local/opt/openblas/lib\" and \"/usr/local/opt/openblas/include\" in the second option with the actual paths where your OpenBLAS library and include files are located. You can find these by running `brew info openblas`.",
         "cached scipy, dependency scipy, install scipy, jupyter bin, scipy_ab2a417aba66481f94d7426bc39eb30d build"
        ],
        [
         "1",
         "51",
         "2025-06-07 22:53:11",
         "ok, redonne moi le code complet du notebook avec la catégorie \"keyword\" et leur génération",
         "Je suis désolé pour la confusion mais il semble que vous cherchiez à créer une base de données SQLite en Python, puis enregistrer des questions utilisateur et réponses du modèle dans cette base. Ensuite, vous avez mentionné une catégorie \"keyword\" qui semble correspondre à des mots-clés ou thèmes de conversation potentiels. \n\nVoici un exemple de code qui implémente cela :\n\n```python\nimport sqlite3\nfrom sqlalchemy import create_engine\nimport random\nimport string\n\n# Pour SQLite\nconn = sqlite3.connect('conversations.db')\ncur = conn.cursor()\n\n# Pour SQLAlchemy\nengine = create_engine('sqlite:///conversations.db')\n\ncur.execute('''\n    CREATE TABLE IF NOT EXISTS conversations  (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        user_input TEXT,\n        llm_output TEXT,\n        keyword TEXT,\n        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n     )\n''')\nconn.commit()\n\n# Générer des mots-clés aléatoires\ndef generate_keyword():\n    return ''.join(random.choice(string.ascii_uppercase) for _ in range(10))\n\nuser_input = \"Votre question ou déclaration d'utilisateur\"\nllm_output = \"Réponse ou déclaration du modèle\"\nkeyword = generate_keyword()  # Génère un mot-clé aléatoirement\n\ncur.execute(\"\"\"\n    INSERT INTO conversations (user_input, llm_output, keyword) VALUES (?, ?, ?)\n\"\"\", (user_input, llm_output, keyword))\nconn.commit()\n\n# Afficher toutes les conversaciones\ncur.execute(\"SELECT * FROM conversations\")\nconversations = cur.fetchall()\nfor conversation in conversations:\n    print(f\"User input: {conversation[0]}\")\n    print(f\"LLM output: {conversation[1]}\")\n    print(f\"Keyword: {conversation[2]}\\n\")  # Affichage du mot-clé\n```\nCe code génère un mot-clé aléatoirement pour chaque conversation, et l'enregistre dans la base de données avec les entrées utilisateur et sorties du modèle. Lorsqu'on récupère toutes les conversations à partir de la base de données, on peut également voir le mot-clé associé à chaque conversation.",
         "create_engine sqlite, sqlalchemy, sqlalchemy engine, sqlite conversations, sqlite3 sqlalchemy"
        ],
        [
         "2",
         "52",
         "2025-06-07 22:53:11",
         "LEs mots clefs doivent être généré a partir des entrées correspondante de la ligne \"question\" et \"réponse\"",
         "D'accord, pour cela, vous pouvez utiliser une approche de machine learning ou de traitement automatique de langue (NLP) pour générer des mots-clés à partir des questions et réponses. Dans ce cas, vous devriez avoir un modèle entrainé sur votre base de données pour extraire les mots clés pertinents.\n\nVoici comment cela pourrait ressembler avec l'utilisation du package `sklearn` et `nltk` (Natural Language Toolkit):\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer \nimport nltk\n\n# Télécharger le dictionnaire de Wordnet pour la normalisation des termes\nnltk.download('wordnet')\nlemmatizer = WordNetLemmatizer() \n\ndef generate_keyword(user_input, llm_output):\n    # Normalise les mots en utilisant lemmatization et crée une liste de tokens\n    user_tokens = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(user_input)]\n    llm_output_tokens = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(llm_output)]\n    \n    # Crée un vecteur TF-IDF \n    vectorizer = TfidfVectorizer()\n    user_vectors = vectorizer.fit_transform([' '.join(user_tokens)])\n    llm_output_vectors = vectorizer.transform([' '.join(llm_output_tokens)])\n    \n    # Calcule la similarité cosinus entre les deux vecteurs et génère un mot-clé \n    similarity = cosine_similarity(user_vectors, llm_output_vectors).flatten()[0]\n    keyword = f\"Keyword_{int(similarity*100)}\"\n    \n    return keyword\n```\nCette fonction utilise la similarité cosinus entre les vecteurs TF-IDF des mots de la question et de la réponse pour calculer une mesure de similarité. Elle génère un mot-clé à partir de cela (en multipliant par 100 pour obtenir un nombre entier). Cependant, c'est une façon simple d'utiliser des mots-clés basés sur la similarité entre les questions et réponses. En production, vous devriez utiliser un modèle de machine learning plus sophistiqué pour générer des mots-clés plus pertinents.\n\nVous pouvez ensuite appeler cette fonction dans votre code existant et l'ajouter comme troisième argument à `cur.execute`:\n\n```python\nkeyword = generate_keyword(user_input, llm_output)  # Génère un mot-clé\ncur.execute(\"\"\"\n    INSERT INTO conversations (user_input, llm_output, keyword) VALUES (?, ?, ?)\n\"\"\",  (user_input, llm_output, keyword))\n```\nNotez que vous devrez probablement installer `nltk` et `sklearn` pour utiliser les packages mentionnés. Vous pouvez faire cela en exécutant `pip install nltk sklearn` dans votre terminal ou invite de commandes.",
         "import nltk, language toolkit, langue nlp, nltk sklearn, termes nltk"
        ],
        [
         "3",
         "53",
         "2025-06-07 22:53:11",
         "Et maintenant comment faire pour automatiser ce script en fond lorsque j'utilise un LLM via LM Studio ?",
         "Pour automatiser le script sur le serveur, vous devriez créer une application qui s'exécute pendant que votre utilisateur interagit avec le modèle de langue naturelle. Cela peut être réalisé par des services comme Google Cloud Functions ou AWS Lambda, qui sont capables d'exécuter du code Python en réponse à certaines événements (comme la réception d'une question).\n\nVoici un exemple de comment vous pouvez automatiser le script sur Google Cloud Functions :\n\n1. Écrivez votre application dans une fonction Python et stockez-la dans un fichier nommé `main.py` par exemple:\n\n```python\nimport os\nfrom google.cloud import firestore\nfrom generate_keyword import generate_keyword  # Assuming your keyword generation function is in this file",
         "automatiser le, cloud functions, cloud import, google cloud, pour automatiser"
        ],
        [
         "4",
         "54",
         "2025-06-07 22:53:11",
         "Je veux que ce soit en local",
         "D'accord, pour exécuter le code localement sans dépendre d'un service cloud, vous pouvez utiliser une base de données locale comme SQLite ou une base NoSQL comme Firebase. Voici comment automatiser votre script avec SQLite dans une application Python:\n\n```python\nimport sqlite3\nfrom generate_keyword import generate_keyword   # Assuming your keyword generation function is in this file\n\n# Pour SQLite\nconn = sqlite3.connect('conversations.db')\ncur = conn.cursor()\n\ndef create_table():\n    cur.execute('''\n        CREATE TABLE IF NOT EXISTS conversations  (\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\n            user_input TEXT,\n            llm_output TEXT,\n            keyword TEXT,\n            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n         )\n     ''')\n    conn.commit()\n\ndef insert_into_table(user_input, llm_output):\n    # Générer le mot-clé à partir des données utilisateur et LLM\n    keyword = generate_keyword(user_input, llm_output)\n    \n    cur.execute(\"\"\"\n        INSERT INTO conversations  (user_input, llm_output, keyword) VALUES  (?, ?, ?)\n    \"\"\",  (user_input, llm_output, keyword))\n    conn.commit()\n\ndef fetch_all():\n    cur.execute(\"SELECT * FROM conversations\")\n    conversations = cur.fetchall()\n    \n    for conversation in conversations:\n        print(f\"User input: {conversation[0]}\")\n        print(f\"LLM output: {conversation[1]}\")\n        print(f\"Keyword: {conversation[2]}\\n\")   # Affichage du mot-clé\n```\n\nNotez que cette implémentation ne fonctionnera pas avec LM Studio, car elle n'est pas compatible. Pour utiliser un modèle de langue naturelle avec une interface utilisateur, vous devrez probablement mettre en œuvre une application web ou mobile (en utilisant des frameworks comme Flask pour le web et React Native ou Swift/Kotlin pour l'application mobile).",
         "conn sqlite3, sqlite, sqlite conn, sqlite3, sqlite3 generate_keyword"
        ],
        [
         "5",
         "44",
         "2025-06-07 22:53:10",
         "Est-ce possible directement avec ce fichier css ?",
         "Oui, il est possible de modifier les styles CSS d'un thème personnalisé pour appliquer différentes configurations à votre coffre Obsidian. Pour cela, vous pouvez créer un fichier CSS dans le répertoire `vault/obsidian/plugins` de votre coffre et ajouter ce fichier en utilisant la feuille de style `cssClass`.\n\nVous avez fourni trois citations qui contiennent des extraits de code CSS. La troisième citation contient un exemple de définition d'une police personnalisée avec les propriétés `font-weight`, `font-style` et `font-display`. Vous pouvez utiliser cette définition pour ajouter une police personnalisée à votre thème.\n\nPour implémenter la troisième citation dans votre fichier CSS personnalisé, suivez ces étapes :\n\n1. Créez un nouveau fichier CSS nommé `<nom_du_fichier>.css` (par exemple, `my-custom-theme.css`) dans le répertoire `vault/obsidian/plugins` de votre coffre. Assurez-vous que ce fichier est enregistré avec l'extension `.css`.\n2. Ajoutez les extraits de code CSS pertinents de la troisième citation à l'intérieur du fichier, par exemple :\n```css\n@font-face {\n    font-family: 'Linux Libertine';\n    src: url('path/to/linux-libertine.woff2') format('woff2'),\n         url('path/to/linux-libertine.woff') format('woff');\n}\n\nbody {\n    font-family: 'Linux Libertine', serif;\n}\n```\n1. Assurez-vous que le chemin vers les fichiers `.woff` et `.woff2` de la police Linux Libertine est correct. Si vous n'avez pas ces fichiers, vous devrez les télécharger séparément. Vous pouvez trouver des liens vers les fichiers `.woff` et `.woff2` sur le site du projet [Linux Libertine](https://github.com/google/fonts/tree/main/apache/linux).\n2. Dans chaque page Markdown où vous souhaitez appliquer votre thème personnalisé, ajoutez le code suivant à la fin du fichier :\n```markdown\n<!--START_IDEAL_VIEW:><html>\n    <head>\n        <!-- Ajoutez cette ligne pour charger votre feuille de style personnalisée -->\n        <link rel=\"stylesheet\" href=\"vault/obsidian/plugins/<nom_du_fichier>.css\">\n    </head>\n</html>\n<!--END_IDEAL_VIEW-->\n```\n1. Enregistrez les fichiers modifiés et redémarrez Obsidian.\n\nMaintenant, chaque page où vous avez ajouté le code ci-dessus devrait afficher votre thème personnalisé avec la police Linux Libertine.",
         "css personnalisé, css pertinents, fichier css, nom_du_fichier css, style cssclass"
        ],
        [
         "6",
         "45",
         "2025-06-07 22:53:10",
         "I want to build a memorizing system for my local LLMs. The idea is to have a SQL base to store the question and the answers, and to have another category with keyword to facilitate the information search. I will provide you the code that I got for the moment. Give me ideas to continue this project : \n\nimport sqlite3\nfrom sqlalchemy import create_engine\n\n# Pour SQLite\nconn = sqlite3.connect('conversations.db')\ncur = conn.cursor()\n\n# Pour SQLAlchemy\nengine = create_engine('sqlite:///conversations.db')\n\ncur.execute('''\n    CREATE TABLE IF NOT EXISTS conversations (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        user_input TEXT,\n        llm_output TEXT,\n        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n    )\n''')\nconn.commit()\n\nuser_input = \"Votre question ou déclaration d'utilisateur\"\nllm_output = \"Réponse ou déclaration du modèle\"\n\ncur.execute(\"\"\"\n    INSERT INTO conversations (user_input, llm_output) VALUES (?, ?)\n\"\"\", (user_input, llm_output))\nconn.commit()\n\ncur.execute(\"SELECT * FROM conversations\")\nconversations = cur.fetchall()\nfor conversation in conversations:\n    print(f\"User input: {conversation[0]}\")\n    print(f\"LLM output: {conversation[1]}\\n\")",
         "You've got a good start with your project, and the SQLite database structure looks appropriate for storing user inputs and their corresponding LLM outputs. Here are some additional ideas to continue this project:\n\n1. **Keyword Search**: You mentioned that you want another category of keywords. This could be used as an index or search feature. You can create a separate table `keywords` with columns `conversation_id` (referring to the conversation), and `keyword`. When storing a new conversation, also extract keywords from your user inputs and store them in this table.\n\n2. **Tagging**: Another idea would be to implement tagging functionality where users can add tags or labels to conversations they find interesting or useful. This could help organize their interactions with the LLM better. You might create a `tags` table that links back to the `conversations` table through its IDs.\n\n3. **Contextual Recommendation**: Based on past conversations, you can implement a recommendation system to suggest relevant past questions and answers. This will help in refining future queries to improve accuracy. You might use natural language processing techniques for this. \n\n4. **User Profiles**: Each user could have a distinct profile with their own set of keywords or tags. They could then search by these profiles rather than having to remember all the keywords from past conversations. This would make searching and retrieving more efficient.\n\n5. **LLM Output Analysis**: Analyze the LLM outputs in natural language processing ways, like sentiment analysis, topic modelling etc., and categorize them based on that for better searchability.\n\n6. **Interactive Learning**: Make your system interactive by allowing users to provide feedback on the quality of responses from the LLMs. Based on these inputs, you can further improve the model's performance over time. \n\n7. **Integration with External APIs or Services**: You could integrate this with external services like translation API to translate user queries into different languages if your users are not all in English and vice versa. Another use case is when your LLM can generate outputs which require external actions like sending emails, making phone calls etc., you could allow for those actions from within the application itself by parsing these instructions out of the generated text.\n\nRemember to always validate user inputs and handle any potential errors that may occur during database operations. Also make sure all data is safely encrypted when storing sensitive information. Good luck with your project !",
         "create_engine sqlite, sqlalchemy, sqlalchemy engine, sqlite conversations, sqlite3 sqlalchemy"
        ],
        [
         "7",
         "46",
         "2025-06-07 22:53:10",
         "Yes, give me an updated version of the code with a \"keywords\" column. The code should implement the use of LM Studio API to find give 10 keyword regarding the Q/A couple with a small LLM.",
         "To achieve this, you will need to install `lms-studio` package in Python and also have access to an LMS Studio account (if it's not available, you can use any other service that provides similar functionality). Here is the updated code:\n\n```python\nimport sqlite3\nfrom sqlalchemy import create_engine\n# from lms-studio import Client  # assuming this package is installed and configured\n\n# Pour SQLite\nconn = sqlite3.connect('conversations.db')\ncur = conn.cursor()\n\n# Pour SQLAlchemy\nengine = create_engine('sqlite:///conversations.db')\n\ncur.execute('''\n    CREATE TABLE IF NOT EXISTS conversations  (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        user_input TEXT,\n        llm_output TEXT,\n        keywords TEXT,\n        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n    )\n''')\nconn.commit()\n\nuser_input = \"Your question or user statement\"\nllm_output = \"Response or statement from the model\"\n# Instantiate a LMS Studio client with your credentials and use it to extract keywords\n# lms = Client(api_key='your-lms-studio-api-key', api_secret='your-api-secret')\n# keywords = lms.extract_keywords(user_input + ' ' + llm_output, limit=10)  # Assuming this function exists\nkeywords = [\"keyword1\", \"keyword2\", ...]  # Replace with your actual list of keywords\n\ncur.execute(\"\"\"\n    INSERT INTO conversations (user_input, llm_output, keywords) VALUES (?, ?, ?)\n\"\"\", (user_input, llm_output, ', '.join(keywords)))\nconn.commit()\n\ncur.execute(\"SELECT * FROM conversations\")\nconversations = cur.fetchall()\nfor conversation in conversations:\n    print(f\"User input: {conversation[0]}\")\n    print(f\"LLM output: {conversation[1]}\")\n    print(f\"Keywords: {conversation[2]}\\n\")  # Assuming that the keywords are stored as comma-separated string in the database. You may want to parse it back into a list or set of strings if you plan on doing keyword searches or other analyses.\n```\nPlease replace `'your-lms-studio-api-key', api_secret='your-api-secret'` with your actual API key and secret from LMS Studio, and also replace the dummy keywords list with actual keyword extraction code using the LMS Studio Python client library.\n\nNote: The `lms-studio` package is not available in PyPI or any other public repository that I know of, so you may need to find another way to extract keywords from text. There are several online services and libraries (like NLTK's word tokenization and frequency distribution functions) that can perform this task.",
         "conn sqlite3, sqlite, sqlite conversations, sqlite3, sqlite3 sqlalchemy"
        ],
        [
         "8",
         "47",
         "2025-06-07 22:53:10",
         "(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Victor ~ % pip install lms-studio\nERROR: Could not find a version that satisfies the requirement lms-studio (from versions: none)\nERROR: No matching distribution found for lms-studio\n(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Victor ~ %",
         "I apologize for the confusion earlier. There is no package called `lms-studio` in PyPI or any other Python package repository. The error message you're seeing typically means that there isn't a valid version of the library available on PyPI with the name and version number specified. \n\nIf you want to extract keywords from text, there are several libraries you can use in Python. One such library is Gensim which has a TF-IDF feature for keyword extraction:\n\n```python\nfrom gensim import models\nfrom nltk.tokenize import word_tokenize\n\nuser_input  ... # same as before\nllm_output  ... # same as before\n\ndoc = user_input + ' ' + llm_output\ntokens = word_tokenize(doc)   # tokenization\ntfidf = models.TfidfModel(tokens)\nwords = [word for word in tokens if len(word) > 5]  # filter out short words, keep only long ones like keywords\nkeywords = max(words, key=tfidf.__getitem__)\n```\nThis is a very basic keyword extraction method and might not work perfectly for your case. For more advanced keyword extraction techniques or services (like those provided by LMS Studio), you may need to find an alternative library/service that fits your needs best. \n\nIf you want to store keywords in the database, you can join them into a string separated by comma: `','.join(keywords)` before inserting them into the database. When retrieving, just split this column on commas to get a list of keywords for each conversation. \n\nAlso note that the Python package name should not include hyphens (-). Replace `lms-studio` with `lmstudio` or something similar if it's actually a package you want to install from PyPI.",
         "install pypi, pypi python, pypi version, studio jupyter, studio pypi"
        ],
        [
         "9",
         "48",
         "2025-06-07 22:53:10",
         "Jupyter) (3.11.3) victorcarre@Mac-mini-de-Victor ~ % python -m pip install gensim\nCollecting gensim\n  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting numpy<2.0,>=1.18.5 (from gensim)\n  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n  Using cached scipy-1.13.1.tar.gz (57.2 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  \n  × Preparing metadata (pyproject.toml) did not run successfully.\n  │ exit code: 1\n  ╰─> [53 lines of output]\n      + meson setup /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-xunjrt5t/scipy_a797efae12854ffc8b9dda38ebad07d1 /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-xunjrt5t/scipy_a797efae12854ffc8b9dda38ebad07d1/.mesonpy-srwpp70c -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-xunjrt5t/scipy_a797efae12854ffc8b9dda38ebad07d1/.mesonpy-srwpp70c/meson-python-native-file.ini\n      The Meson build system\n      Version: 1.8.1\n      Source dir: /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-xunjrt5t/scipy_a797efae12854ffc8b9dda38ebad07d1\n      Build dir: /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-xunjrt5t/scipy_a797efae12854ffc8b9dda38ebad07d1/.mesonpy-srwpp70c\n      Build type: native build\n      Project name: scipy\n      Project version: 1.13.1\n      C compiler for the host machine: cc (clang 17.0.0 \"Apple clang version 17.0.0 (clang-1700.0.13.3)\")\n      C linker for the host machine: cc ld64 1167.4.1\n      C++ compiler for the host machine: c++ (clang 17.0.0 \"Apple clang version 17.0.0 (clang-1700.0.13.3)\")\n      C++ linker for the host machine: c++ ld64 1167.4.1\n      Cython compiler for the host machine: cython (cython 3.0.12)\n      Host machine cpu family: aarch64\n      Host machine cpu: aarch64\n      Program python found: YES (/Users/victorcarre/venvs/Jupyter/bin/python)\n      Found pkg-config: YES (/opt/homebrew/bin/pkg-config) 2.4.3\n      Run-time dependency python found: YES 3.13\n      Program cython found: YES (/private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-build-env-cc0r6zlv/overlay/bin/cython)\n      Compiler for C supports arguments -Wno-unused-but-set-variable: YES\n      Compiler for C supports arguments -Wno-unused-function: YES\n      Compiler for C supports arguments -Wno-conversion: YES\n      Compiler for C supports arguments -Wno-misleading-indentation: YES\n      Library m found: YES\n      \n      ../meson.build:78:0: ERROR: Unknown compiler(s): [['gfortran'], ['flang-new'], ['flang'], ['nvfortran'], ['pgfortran'], ['ifort'], ['ifx'], ['g95']]\n      The following exception(s) were encountered:\n      Running `gfortran --help` gave \"[Errno 2] No such file or directory: 'gfortran'\"\n      Running `gfortran --version` gave \"[Errno 2] No such file or directory: 'gfortran'\"\n      Running `gfortran -V` gave \"[Errno 2] No such file or directory: 'gfortran'\"\n      Running `flang-new --help` gave \"[Errno 2] No such file or directory: 'flang-new'\"\n      Running `flang-new --version` gave \"[Errno 2] No such file or directory: 'flang-new'\"\n      Running `flang-new -V` gave \"[Errno 2] No such file or directory: 'flang-new'\"\n      Running `flang --help` gave \"[Errno 2] No such file or directory: 'flang'\"\n      Running `flang --version` gave \"[Errno 2] No such file or directory: 'flang'\"\n      Running `flang -V` gave \"[Errno 2] No such file or directory: 'flang'\"\n      Running `nvfortran --help` gave \"[Errno 2] No such file or directory: 'nvfortran'\"\n      Running `nvfortran --version` gave \"[Errno 2] No such file or directory: 'nvfortran'\"\n      Running `nvfortran -V` gave \"[Errno 2] No such file or directory: 'nvfortran'\"\n      Running `pgfortran --help` gave \"[Errno 2] No such file or directory: 'pgfortran'\"\n      Running `pgfortran --version` gave \"[Errno 2] No such file or directory: 'pgfortran'\"\n      Running `pgfortran -V` gave \"[Errno 2] No such file or directory: 'pgfortran'\"\n      Running `ifort --help` gave \"[Errno 2] No such file or directory: 'ifort'\"\n      Running `ifort --version` gave \"[Errno 2] No such file or directory: 'ifort'\"\n      Running `ifort -V` gave \"[Errno 2] No such file or directory: 'ifort'\"\n      Running `ifx --help` gave \"[Errno 2] No such file or directory: 'ifx'\"\n      Running `ifx --version` gave \"[Errno 2] No such file or directory: 'ifx'\"\n      Running `ifx -V` gave \"[Errno 2] No such file or directory: 'ifx'\"\n      Running `g95 --help` gave \"[Errno 2] No such file or directory: 'g95'\"\n      Running `g95 --version` gave \"[Errno 2] No such file or directory: 'g95'\"\n      Running `g95 -V` gave \"[Errno 2] No such file or directory: 'g95'\"\n      \n      A full log can be found at /private/var/folders/g5/gfzgn6710bl4fnkqby736jyh0000gn/T/pip-install-xunjrt5t/scipy_a797efae12854ffc8b9dda38ebad07d1/.mesonpy-srwpp70c/meson-logs/meson-log.txt\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─> See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Victor ~ %",
         "It seems there's an issue with the scipy package installation in your environment because of missing Fortran compiler dependencies. The error messages suggest that some Fortran compilers are not found on your system. This is a common problem when installing scientific packages like Gensim which rely heavily on numerical and scientific computation libraries.\n\nThere are several solutions to this issue:\n1. Install the required Fortran compilers on your machine. If you're using Homebrew, you can install them with `brew install gcc` or `brew install gfortran`.\n2. Use a Python virtual environment and try installing the package there. You might need to add the path of the GCC installed by Homebrew to the PATH environment variable in your shell if it's not found automatically.\n3. If you don't need all functionalities of scipy, consider installing only gensim with `pip install gensim` and skip scipy for now.\n4. Install GCC and Fortran compilers as a system package (not recommended if you can avoid it).\n5. Consider using Anaconda Python distribution which is prepackaged with most of the scientific packages including numpy, scipy, etc., so you might not face these installation issues. \n\nIf none of above solutions works for you, please provide more details about your environment (OS, Python version) so I could give a more targeted advice.",
         "cached scipy, jupyter bin, scipy install, scipy package, scipy_a797efae12854ffc8b9dda38ebad07d1 build"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_input</th>\n",
       "      <th>llm_output</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>2025-06-07 22:53:11</td>\n",
       "      <td>(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Vic...</td>\n",
       "      <td>It seems like there's a problem with the OpenB...</td>\n",
       "      <td>cached scipy, dependency scipy, install scipy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>2025-06-07 22:53:11</td>\n",
       "      <td>ok, redonne moi le code complet du notebook av...</td>\n",
       "      <td>Je suis désolé pour la confusion mais il sembl...</td>\n",
       "      <td>create_engine sqlite, sqlalchemy, sqlalchemy e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>2025-06-07 22:53:11</td>\n",
       "      <td>LEs mots clefs doivent être généré a partir de...</td>\n",
       "      <td>D'accord, pour cela, vous pouvez utiliser une ...</td>\n",
       "      <td>import nltk, language toolkit, langue nlp, nlt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>2025-06-07 22:53:11</td>\n",
       "      <td>Et maintenant comment faire pour automatiser c...</td>\n",
       "      <td>Pour automatiser le script sur le serveur, vou...</td>\n",
       "      <td>automatiser le, cloud functions, cloud import,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>2025-06-07 22:53:11</td>\n",
       "      <td>Je veux que ce soit en local</td>\n",
       "      <td>D'accord, pour exécuter le code localement san...</td>\n",
       "      <td>conn sqlite3, sqlite, sqlite conn, sqlite3, sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44</td>\n",
       "      <td>2025-06-07 22:53:10</td>\n",
       "      <td>Est-ce possible directement avec ce fichier css ?</td>\n",
       "      <td>Oui, il est possible de modifier les styles CS...</td>\n",
       "      <td>css personnalisé, css pertinents, fichier css,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45</td>\n",
       "      <td>2025-06-07 22:53:10</td>\n",
       "      <td>I want to build a memorizing system for my loc...</td>\n",
       "      <td>You've got a good start with your project, and...</td>\n",
       "      <td>create_engine sqlite, sqlalchemy, sqlalchemy e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>46</td>\n",
       "      <td>2025-06-07 22:53:10</td>\n",
       "      <td>Yes, give me an updated version of the code wi...</td>\n",
       "      <td>To achieve this, you will need to install `lms...</td>\n",
       "      <td>conn sqlite3, sqlite, sqlite conversations, sq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47</td>\n",
       "      <td>2025-06-07 22:53:10</td>\n",
       "      <td>(Jupyter) (3.11.3) victorcarre@Mac-mini-de-Vic...</td>\n",
       "      <td>I apologize for the confusion earlier. There i...</td>\n",
       "      <td>install pypi, pypi python, pypi version, studi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>2025-06-07 22:53:10</td>\n",
       "      <td>Jupyter) (3.11.3) victorcarre@Mac-mini-de-Vict...</td>\n",
       "      <td>It seems there's an issue with the scipy packa...</td>\n",
       "      <td>cached scipy, jupyter bin, scipy install, scip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            timestamp                                         user_input  \\\n",
       "0  50  2025-06-07 22:53:11  (Jupyter) (3.11.3) victorcarre@Mac-mini-de-Vic...   \n",
       "1  51  2025-06-07 22:53:11  ok, redonne moi le code complet du notebook av...   \n",
       "2  52  2025-06-07 22:53:11  LEs mots clefs doivent être généré a partir de...   \n",
       "3  53  2025-06-07 22:53:11  Et maintenant comment faire pour automatiser c...   \n",
       "4  54  2025-06-07 22:53:11                       Je veux que ce soit en local   \n",
       "5  44  2025-06-07 22:53:10  Est-ce possible directement avec ce fichier css ?   \n",
       "6  45  2025-06-07 22:53:10  I want to build a memorizing system for my loc...   \n",
       "7  46  2025-06-07 22:53:10  Yes, give me an updated version of the code wi...   \n",
       "8  47  2025-06-07 22:53:10  (Jupyter) (3.11.3) victorcarre@Mac-mini-de-Vic...   \n",
       "9  48  2025-06-07 22:53:10  Jupyter) (3.11.3) victorcarre@Mac-mini-de-Vict...   \n",
       "\n",
       "                                          llm_output  \\\n",
       "0  It seems like there's a problem with the OpenB...   \n",
       "1  Je suis désolé pour la confusion mais il sembl...   \n",
       "2  D'accord, pour cela, vous pouvez utiliser une ...   \n",
       "3  Pour automatiser le script sur le serveur, vou...   \n",
       "4  D'accord, pour exécuter le code localement san...   \n",
       "5  Oui, il est possible de modifier les styles CS...   \n",
       "6  You've got a good start with your project, and...   \n",
       "7  To achieve this, you will need to install `lms...   \n",
       "8  I apologize for the confusion earlier. There i...   \n",
       "9  It seems there's an issue with the scipy packa...   \n",
       "\n",
       "                                            keywords  \n",
       "0  cached scipy, dependency scipy, install scipy,...  \n",
       "1  create_engine sqlite, sqlalchemy, sqlalchemy e...  \n",
       "2  import nltk, language toolkit, langue nlp, nlt...  \n",
       "3  automatiser le, cloud functions, cloud import,...  \n",
       "4  conn sqlite3, sqlite, sqlite conn, sqlite3, sq...  \n",
       "5  css personnalisé, css pertinents, fichier css,...  \n",
       "6  create_engine sqlite, sqlalchemy, sqlalchemy e...  \n",
       "7  conn sqlite3, sqlite, sqlite conversations, sq...  \n",
       "8  install pypi, pypi python, pypi version, studi...  \n",
       "9  cached scipy, jupyter bin, scipy install, scip...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = '''\n",
    "SELECT c.id, c.timestamp, c.user_input, c.llm_output, GROUP_CONCAT(k.keyword, ', ') AS keywords\n",
    "FROM conversations c\n",
    "LEFT JOIN keywords k ON c.id = k.conversation_id\n",
    "GROUP BY c.id\n",
    "ORDER BY c.timestamp DESC\n",
    "LIMIT 50\n",
    "'''\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de requête SQL\n",
    "\n",
    "```sql\n",
    "SELECT c.user_input, c.llm_output, GROUP_CONCAT(k.keyword, ', ') as keywords\n",
    "FROM conversations c\n",
    "LEFT JOIN keywords k ON c.id = k.conversation_id\n",
    "GROUP BY c.id\n",
    "ORDER BY c.timestamp DESC\n",
    "LIMIT 10;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLChem (venv-jupyter)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
